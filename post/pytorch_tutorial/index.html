<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Pytorch Tutorial - Jijeng&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="jijeng" /><meta name="description" content="pytorch 学习笔记
不理解的地方，一个是关于 nn包搭建网络的部分。
" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.79.1 with theme even" />


<link rel="canonical" href="http://jijeng.github.io/post/pytorch_tutorial/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Pytorch Tutorial" />
<meta property="og:description" content="pytorch 学习笔记
不理解的地方，一个是关于 nn包搭建网络的部分。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://jijeng.github.io/post/pytorch_tutorial/" />
<meta property="article:published_time" content="2019-02-01T16:11:50+08:00" />
<meta property="article:modified_time" content="2019-02-01T16:11:50+08:00" />
<meta itemprop="name" content="Pytorch Tutorial">
<meta itemprop="description" content="pytorch 学习笔记
不理解的地方，一个是关于 nn包搭建网络的部分。">
<meta itemprop="datePublished" content="2019-02-01T16:11:50+08:00" />
<meta itemprop="dateModified" content="2019-02-01T16:11:50+08:00" />
<meta itemprop="wordCount" content="13207">



<meta itemprop="keywords" content="pytorch," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Pytorch Tutorial"/>
<meta name="twitter:description" content="pytorch 学习笔记
不理解的地方，一个是关于 nn包搭建网络的部分。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Jijeng&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Jijeng&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Pytorch Tutorial</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-02-01 </span>
        <div class="post-category">
            <a href="/categories/tutorial/"> tutorial </a>
            </div>
          <span class="more-meta"> 约 13207 字 </span>
          <span class="more-meta"> 预计阅读 27 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#可视化">可视化</a></li>
            <li><a href="#pytorch-单机多卡操作总结分布式dataparallel混合精度horovodhttpsjishuinproginncomp763bfbd2f6b6"><a href="https://jishuin.proginn.com/p/763bfbd2f6b6">PyTorch 单机多卡操作总结：分布式DataParallel，混合精度，Horovod</a></a></li>
            <li><a href="#分布式计算调研">分布式计算调研</a></li>
            <li><a href="#拷贝">拷贝</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>pytorch 学习笔记</p>
<p>不理解的地方，一个是关于 nn包搭建网络的部分。</p>
<ol>
<li>pytorch 和torch 比较</li>
</ol>
<p>编程语言：
pytorch 采用python语言，实际上使用c语言和c++ 做接口
torch 采用lua，使用c语言和lua 语言做接口
（lua 语言相当于一个小型加强版的c语言，支持类和面向对象）
依赖库：
pytorch 和 torch 框架的区别和联系：
pytorch 可调用python强大的第三方库，比如 opencv
torch 可调用 lua 库函数，目前 lua库函数没有python多
pytorch 依赖库多于 torch
效率：
python 的debug 功能比lua 强大，所以pytorch 效率高于torch
模型和中间变量的关系：
pytorch 中中间变量都存在计算图中，所以model 共享中间变量
torch 的中间变量在每一个模块中，所以想要调用其他模块的参数就必须复制这个模块然后再调用</p>
<p>总结
pytorch可以说是torch 的python版本，并增加了很多新功能</p>
<ol start="2">
<li>常用的框架比较</li>
</ol>
<p>tensorflow 背后是google， mxnet 是Amazon，pytorch背后是Facebook
每个框架都有各自的有点，比如tensorflow的工程能力很强，Theano特别适合科研等等
keras是一个很高层的结构，它的后端支持theano和tensorflow，它本质上并不是一个框架，只是对框架的操作做了一个封装，你在写keras的时候其实是对其后端进行调用，相当于你还是在tensorflow或者theano上跑程序，只不过你把你的语言交给keras处理了一下变成tensorflow听得懂的语言，然后再交给tensorflow处理，这样的后果当然方便你构建网络，方便定义模型做训练，极快的构建你的想法，工程实现很强，但是这样也有一个后果，那就是细节你没有办法把控，训练过程高度封装，导致你没有办法知道里面的具体细节，以及每个参数的具体细节，使得调试和研究变得很困难。</p>
<ol start="3">
<li>pytorch的思想</li>
</ol>
<p>PyTorch 的构建者表明，PyTorch 的哲学是解决当务之急，也就是说即时构建和运行我们的计算图。这恰好适合 Python 的编程方法，因为我们不需等待整个代码都被写入才能知道是否起作用。我们很容易运行部分代码，并实时检查它。</p>
<p>PyTorch 是一个基于 Python 的库，旨在为深度学习提供一个灵活的开发平台。PyTorch 的工作流程非常接近于 Python 的科学计算库 NumPy。那么为什么我们需要使用 PyTorch 构建深度学习模型？以下作者根据实际经验提供了三个理由：</p>
<ul>
<li>便于使用的 API：它的使用如同 Python 那样简单。</li>
<li>支持 Python：正如上文所述，PyTorch 可以平滑地与 Python 数据科学栈相结合。它与 NumPy 一样简单，甚至我们都感觉不出它们的区别。</li>
<li>动态计算图：PyTorch 不再采用特定的函数预定义计算图，而是提供构建动态计算图的框架，甚至我们可以在运行时修正它们。这种动态框架在我们不知道所构建的神经网络需要多少内存时非常有用。
其它一些使用 PyTorch 的优点还有多 GPU 支持、自定义数据加载器和极简的预处理过程等。</li>
</ul>
<p>在讨论 PyTorch 的各个组件前，我们需要了解它的工作流。PyTorch 使用一种称之为 imperative / eager 的范式，即每一行代码都要求构建一个图以定义完整计算图的一个部分。即使完整的计算图还没有完成构建，我们也可以独立地执行这些作为组件的小计算图，这种动态计算图被称为**「define-by-run」**方法。</p>
<p>PyTorch 提供了 CPU 张量和 GPU 张量，并且极大地加速了计算的速度。
从张量的构建与运行就能体会到 PyTorch 相比 TensorFLow 需要声明张量、初始化张量要简洁地多。以下语句将随机初始化一个 5×3 的二维张量，因为 PyTorch 是一种动态图，所以它声明和真实赋值是同时进行的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">torch.Tensor(5, 3)
</code></pre></td></tr></table>
</div>
</div><p>若我们希望随机初始化的张量服从某些分布，那么我们可以直接对张量对象使用一些方法。如下初始化的张量将服从均匀分布：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">torch.Tensor(5, 3).uniform_(-1, 1)
</code></pre></td></tr></table>
</div>
</div><p>PyTorch 同样支持广播（Broadcasting）操作，一般它会隐式地把一个数组的异常维度调整到与另一个算子相匹配的维度以实现维度兼容。</p>
<p>如下，我们定义了两个 GPU 张量，并对这两个张量执行矩阵乘法。当然，我们也可以如下所示将 CPU 张量转换为 GPU 张量。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 以下转化CPU张量为GPU张量</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><code>AutoGrad 模块</code></p>
<p>TensorFlow、Caffe 和 CNTK 等大多数框架都是使用的静态计算图，开发者必须建立或定义一个神经网络，并重复使用相同的结构来执行模型训练。改变网络的模式就意味着我们必须从头开始设计并定义相关的模块。</p>
<p>PyTorch 使用的技术为自动微分（automatic differentiation），这个是用来自动求解微分的模块。在这种机制下，系统会有一个 Recorder 来记录我们执行的运算，然后再反向计算对应的梯度。这种技术在构建神经网络的过程中十分强大，因为我们可以通过计算前向传播过程中参数的微分来节省时间。</p>
<p>从概念上讲, <code>Autograd</code> 对数据记录记录了一个有向无环图（DAG）， 叫做计算图，用来表示它的计算过程。沿着计算图应用链式求导法则就可以求出其梯度。<code>Autograd</code> 包中有两个核心包： <code>torch.Tensor</code> 和<code>torch.Function</code>， 默认某个 tensor的属性是 <code>.requires_grad</code> 为true，当计算完成的时候可以调用 <code>.backward()</code> 来自动计算所有的梯度，针对这个tensor 可以在 <code>.grad</code> 属性中去查看。</p>
<p>设置一个张量不跟踪历史记录的方法：</p>
<ol>
<li>调用 <code>.detach()</code> 将其从计算历史中分离出来</li>
<li>使用 <code>torch.no_grad()</code> 包裹代码块，那么在该代码块中的计算都不会计算梯度。使用的情况是， 在评估阶段（predict）阶段。</li>
<li>设置某个tensor 的属性为 <code>Required_grad =False</code></li>
</ol>
<p><code>Function</code> 类，每一个tensor都有一个<code>.grad_fn</code> 属性指向一个 <code>Function</code>，表示如何得到了当期的tensor。如果是用户自己创建的张量tensor，那么 <code>grad_fn is None</code>。</p>
<p><code>.requires_grad</code>具有传递性，比如说 $x_1, x_2, \dots, x_n$ 中某一个满足 <code>required_grad =True</code>，那么这个时候由这些tensor表示tensor 的属性都是<code>true</code>。</p>
<p><strong>最优化模块</strong>
torch.optim 是实现神经网络中多种优化算法的模块，它目前已经支持大多数一般的方法，所以我们不需要从头构建优化算法。以下展示了使用 Adam 优化器的基本代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
</code></pre></td></tr></table>
</div>
</div><p>我们一般可以使用 torch.nn 包构建神经网络，下面提供了一些 API 的表达及意义：</p>
<ul>
<li>线性层- nn.Linear、nn.Bilinear</li>
<li>卷积层 - nn.Conv1d、nn.Conv2d、nn.Conv3d、nn.ConvTranspose2d</li>
<li>非线性激活函数- nn.Sigmoid、nn.Tanh、nn.ReLU、nn.LeakyReLU</li>
<li>池化层 - nn.MaxPool1d、nn.AveragePool2d</li>
<li>循环网络 - nn.LSTM、nn.GRU</li>
<li>归一化 - nn.BatchNorm2dDropout - nn.Dropout、nn.Dropout2d</li>
<li>嵌入 - nn.Embedding</li>
<li>损失函数 - nn.MSELoss、nn.CrossEntropyLoss、nn.NLLLoss</li>
</ul>
<p><strong>张量</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">import torch
a = torch.FloatTensor(5, 7)
</code></pre></td></tr></table>
</div>
</div><p>相同点 / 不同点
第一个区别是，所有的操作在张量操作需要有_后缀。例如，add在此处无用，使用add_是可用的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">a</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">3.5</span><span class="p">)</span>
<span class="c1"># 将a填充值3.5。</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
<span class="c1"># a 依然是填充3.5</span>
<span class="c1"># 新张量b的返回值为3.5 + 4＝7.5。</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>零索引</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">b = a[0, 3]  # 选择a中的1行4列
b = a[:, 3:5]  # 从a中选择所以行的4到5列
x.index_add_(1, torch.LongTensor([4, 0]), z)
</code></pre></td></tr></table>
</div>
</div><p>下一个小的区别是所有的功能现在都不是驼峰命名了。例如indexAdd现在调用index_add_</p>
<p>CUDA传感器在pytorch中很好并且很容易，并将CUDA张量从CPU转移到GPU将保留其基础类型。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查看电脑是否支持CUDA</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="c1"># 创建一个LongTensor并且把它全部转换为3</span>
    <span class="c1"># to GPU as torch.cuda.LongTensor</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="c1"># transfers it to CPU, back to</span>
    <span class="c1"># being a torch.LongTensor</span>
</code></pre></td></tr></table>
</div>
</div><p>基本类型
Tensor的基本数据类型有五种：</p>
<ul>
<li>32位浮点型：torch.FloatTensor。 (默认)</li>
<li>64位整型：torch.LongTensor。</li>
<li>32位整型：torch.IntTensor。</li>
<li>16位整型：torch.ShortTensor。</li>
<li>64位浮点型：torch.DoubleTensor。</li>
</ul>
<p>numpy 和 tensor 之间的相互转换</p>
<p>使用numpy 方法将tensor 转换成 ndarray</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="c1"># tensor转化为numpy</span>
<span class="n">numpy_a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">numpy_a</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>numpy转化为Tensor</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">torch_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">numpy_a</span><span class="p">)</span>
<span class="n">torch_a</span>
</code></pre></td></tr></table>
</div>
</div><p>一般情况下可以使用.cuda方法将tensor移动到gpu，这步操作需要cuda设备支持</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">cpu_a=torch.rand(4, 3)
cpu_a.type()
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">gpu_a=cpu_a.cuda()
gpu_a.type()
</code></pre></td></tr></table>
</div>
</div><p>使用.cpu 将tensor 转换成cpu</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">cpu_b=gpu_a.cpu()
cpu_b.type()
</code></pre></td></tr></table>
</div>
</div><p>如果我们有多GPU的情况，可以使用to方法来确定使用那个设备，这里只做个简单的实例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">#使用torch.cuda.is_available()来确定是否有cuda设备
device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
print(device)
#将tensor传送到设备
gpu_b=cpu_b.to(device)
gpu_b.type()
</code></pre></td></tr></table>
</div>
</div><p>下一章介绍PyTorch的自动求导机制</p>
<ul>
<li>变量</li>
<li>梯度</li>
</ul>
<p>从0.4起, Variable 正式合并入Tensor类, 通过Variable嵌套实现的自动微分功能已经整合进入了Tensor类中。虽然为了代码的兼容性还是可以使用Variable(tensor)这种方式进行嵌套, 但是这个操作其实什么都没做。所以，以后的代码建议直接使用Tensor类进行操作，因为官方文档中已经将Variable设置成过期模块。要想通过Tensor类本身就支持了使用autograd功能，只需要设置.requries_grad=True。 Variable类中的的grad和grad_fn属性已经整合进入了Tensor类中</p>
<p>每个变量都有两个标志：<code>requires_grad</code>和<code>volatile</code>。它们都允许从梯度计算中精细地排除子图，并可以提高效率。</p>
<p><code>requires_grad</code>
如果有一个单一的输入操作需要梯度，它的输出也需要梯度。相反，只有所有输入都不需要梯度，输出才不需要。如果其中所有的变量都不需要梯度进行，后向计算不会在子图中执行。</p>
<p>这个标志特别有用，当您想要冻结部分模型时，或者您事先知道不会使用某些参数的梯度。例如，如果要对预先训练的CNN进行优化，只要切换冻结模型中的requires_grad标志就足够了，直到计算到最后一层才会保存中间缓冲区，其中的仿射变换将使用需要梯度的权重并且网络的输出也将需要它们。</p>
<p><code>volatile</code></p>
<p>纯粹的inference模式下推荐使用volatile，当你确定你甚至不会调用.backward()时。它比任何其他自动求导的设置更有效——它将使用绝对最小的内存来评估模型。volatile也决定了require_grad is False。</p>
<p>volatile不同于require_grad的传递。如果一个操作甚至只有有一个volatile的输入，它的输出也将是volatile。Volatility比“不需要梯度”更容易传递——只需要一个volatile的输入即可得到一个volatile的输出，相对的，需要所有的输入“不需要梯度”才能得到不需要梯度的输出。使用volatile标志，您不需要更改模型参数的任何设置来用于inference。创建一个volatile的输入就够了，这将保证不会保存中间状态。</p>
<p>参考<a href="https://pytorch-cn.readthedocs.io/zh/latest/notes/autograd/">官方教程</a></p>
<p>PyTorch 基础 : 神经网络包nn和优化器optm</p>
<p><strong>pytorch 学习笔记</strong></p>
<ol>
<li>pytorch 的核心主要是提供了两个主要的功能：</li>
</ol>
<ul>
<li>n维tensor，类似numpy，但可以运行在GPU 上
Numpy是科学计算的通用框架;它对计算图形、深度学习或梯度一无所知。Tensor张量是pytorch 中最基本的概念。PyTorch张量可以利用GPU加速其数字计算。要在GPU上运行PyTorch Tensor，请在构造Tensor时使用<code>device</code>参数将Tensor放置在GPU上。</li>
<li>自动微分，用于构建和训练神经网络
使用自动微分来自动计算神经网络中的反向通过。</li>
</ul>
<ol start="2">
<li>
<p>在搭建网络的过程中，网络中的正向传播定义为一个 computational graph计算图： 图中的节点为张量，边为从输入张量产生输出张量的函数，然后通过改图进行反向传播，可以轻松计算梯度。默认张量中的参数 <code>require_grad=True</code>， 那么在反向传播的时候， <code>x.grad</code>将是另一个张量， 它保持了<code>x</code> 相对于某个标量值的梯度。如果在训练神经网络的时候，比如通常不想要更新步骤中向后传播（形成计算图），那么这个时候可以使用 <code>torch.no_grad()</code> 上下文管理器来防止构建计算图。</p>
</li>
<li>
<p>pytorch 中的计算图很想 tensorflow 中的计算图，但是两者不同在于前者是动态图，后者是静态图。在tensorflow 中，如果定义了一个计算图，然后一遍遍计算相同的图，可能将不同的输入数据提供给图。在pytorch 中，每一个前向传播都定义了一个新的计算图。静态图的优势，可以预先优化图，比如说融合某些图的操作，分布式之类的。而动态图，入门比较简单，方便debug。</p>
</li>
<li>
<p>pytorch中常见的包：</p>
</li>
</ol>
<ul>
<li>nn 定义了一组模块，包括神经网络层和有用的损失函数</li>
<li>optim（优化器），优化算法的思想， 比如说 adagrad， rmsprop， adam</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell"><span class="c1"># 常见的操作</span>
调用模型前向传播 <span class="nv">y_pred</span> <span class="o">=</span> model<span class="o">(</span>x<span class="o">)</span>
调用损失函数 <span class="nv">loss</span> <span class="o">=</span> loss_fn<span class="o">(</span>y_pred, y<span class="o">)</span>
梯度归零 optimizer.zero_grad<span class="o">()</span>
后向传播 loss.backward<span class="o">()</span>
调用优化器更新参数 optimizer.step<span class="o">()</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">In PyTorch, we need to <span class="nb">set</span> the gradients to zero before starting to <span class="k">do</span> backpropragation because PyTorch accumulates the gradients on subsequent backward passes. This is convenient <span class="k">while</span> training RNNs. So, the default action is to accumulate <span class="o">(</span>i.e. sum<span class="o">)</span> the gradients on every loss.backward<span class="o">()</span> call.

Because of this, when you start your training loop, ideally you should zero out the gradients so that you <span class="k">do</span> the parameter update correctly. Else the gradient would point in some other direction than the intended direction towards the minimum <span class="o">(</span>or maximum, in <span class="k">case</span> of maximization objectives<span class="o">)</span>.
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>pytorch 中的梯度计算是累积的，这个在训练 RNN 时候很方便。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># pytorch 中的 optim 和 loss 的交互
# optimizer 获取了素有 parameter 的引用，每个parameter 都包含梯度（gradient）
optimizer = torch.optim.SGD(net.parameters(), lr =0.5) 

# 下面这两句话产生梯度。第二句话是将梯度反向传播到整个网络中的所有节点
loss =loss_func(prediction, y)
loss.backward()

# apply 所有的梯度更新parameter 的值
optimizer.step()

</code></pre></td></tr></table>
</div>
</div><ol>
<li>
<p>使用 custom nn module（这种是经常用到，搭建自己的nn 网络）
通过子类nn.Module并定义一个<code>forwad</code>输入来定义自己的模块，该前向接收输入张量并使用其他模块或在张量上的其他自动转换操作产生输出张量。</p>
</li>
<li>
<p>control flow and weight sharing
这个权值共享在RNN 中使用比较多，但是不是很多呀，多看例子把~</p>
</li>
<li>
<p>pytorch  中 <code>model.train()</code> 和 <code>model.eval()</code> 的区别</p>
</li>
</ol>
<p>在图像中影响比较大。如果模型中出现了BatchNormalization 和Dropout，必须要区分训练和验证模式。两种模式在计算上是不同的。当 eval 的时候，模型会自动把 BN和DropOut固定住，使用已经训练好的值，否则容易出现异常的结果。</p>
<ol start="8">
<li>contiguous 关键词</li>
</ol>
<p>contiguous  使用空间换取时间，保证语义上相邻的元素在内存上也是连续的。这样访问的时候，可以减少cpu 对内存的请求的次数。</p>
<p><code>torch.Tensor</code> 和 <code>torch.tensor</code> 的区别</p>
<blockquote>
<p>In Pytorch <code>torch.Tensor</code> is the main tensor class. So all tensors are just instances of <code>torch.Tensor</code></p>
<p>When you call <code>torch.Tensor()</code> you will get an empty tensor without any data 是一个类</p>
<p>on ontrast <code>torch.tensor</code> is a function which results a tensor.  是一个函数</p>
</blockquote>
<p>tensor 和 variable 的区别</p>
<blockquote>
<p>tensor and variable are a class provided by Pytorch. Accordign to the official Pytorch document. Both classes are a multi-dimensional matrix containing elements of a single data type （类似 numpy 中的 array）</p>
<p>The  difference between Tensor and Variable is that the Variable is a wrapper of Tensor（当初主要目的是 variable 进行包含了自动求导，tensor 没有自动求导的功能）</p>
<p>however, as of now the Variable class has been deprecated and we are no longer bother between Variable and Tensor since the Autograd also supports Tensor.</p>
</blockquote>
<p>replay 重放</p>
<h3 id="可视化">可视化</h3>
<p> PyTorch是Torch框架的表亲，Torch是基于lua开发的，在Facebook公司里被广泛使用。然而，PyTorch的出现并不是为了支持流行语言而对Torch进行简单的包装，它被重写和定制出来是为了得到更快的速度和本地化。</p>
<p>tensorboardx</p>
<p>Visdom是Facebook在2017年发布的一款针对PyTorch的可视化工具</p>
<p>方案： pytorch 使用替代品tensorbordx</p>
<p>Pytorch框架也有自己的可视化软件&ndash;Visdom，但貌似不是很好用。所以是可以用tensorboardx 来进行运行的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorboardX</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">tensorboard</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow</span>

<span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">runs</span>
<span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">6006</span><span class="o">/</span>    <span class="c1">#在chrome浏览器中打开</span>

</code></pre></td></tr></table>
</div>
</div><p>注意numpy的版本要对应，否则会报错，如果不匹配，那就进行更新或者新建虚拟环境了！</p>
<ol>
<li>Loss可视化
最常见的可视化就是loss曲线作图</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">tb_logger</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;loss_train&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">curr_step</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>输入图片和标签的可视化</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tb_logger.add_image(&#39;image&#39;, input[0], curr_step)
</code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>单通道特征图的可视化</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">def make_grid(tensor, nrow=8, padding=2,
              normalize=False, range=None, scale_each=False, pad_value=0):
</code></pre></td></tr></table>
</div>
</div><p>pytorch最被人诟病的就是可视化问题和部署问题</p>
<p><a href="https://shenxiaohai.me/2018/10/23/pytorch-tutorial-TensorBoard/">https://shenxiaohai.me/2018/10/23/pytorch-tutorial-TensorBoard/</a></p>
<p><a href="https://www.jianshu.com/p/429eb27855a0">https://www.jianshu.com/p/429eb27855a0</a></p>
<p>可视化工具 visdom</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">pip install visdom
</code></pre></td></tr></table>
</div>
</div><p>使用 以下命令在本地启动服务器</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">python -m visdom.server 
</code></pre></td></tr></table>
</div>
</div><p>然后输入 <code>http://localhost:8097</code></p>
<p>model.eval() 和 model.no_grad()  的区别</p>
<p>model.eval()</p>
<blockquote>
<ul>
<li>在train模式，dropout层会按照设定的参数p设置保留激活单元的概率（保留概率=p，比如keep_prob=0.8），batchnorm层会继续计算数据的mean和var并进行更新</li>
<li>在val模式下，dropout层会让所有的激活单元都通过，而batchnorm层会停止计算和更新mean和var，直接使用在训练阶段已经学出的mean和var值</li>
<li>model.eval()不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反向传播(backprobagation)</li>
</ul>
</blockquote>
<p>model.no_grad()</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span><span class="err">：</span>
	<span class="c1"># 代码块</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<ul>
<li>用于停止autograd模块的工作，起到加速和节省显存的作用（具体行为就是停止gradient计算，从而节省了GPU算力和显存）</li>
<li>不会影响dropout和batchnorm层的行为</li>
</ul>
</blockquote>
<p>所以在部署的时候，需要两者搭配起来进行使用，这样可以处理 dropout 和 BN 的情况，也可减少显存的使用。</p>
<blockquote>
<p>These two have different goals:
model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.
torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).
eval()和train 的不同在于 BN 和DropOut 机制的是否使用，前者是不使用，后者是使用。 no_grad() 是不进行反向传播，主要是用来减少内存和加快训练的。</p>
</blockquote>
<blockquote>
<p>Dropout works as a regularization for preventing overfitting during training.
It randomly zeros the elements of inputs in Dropout layer on forward call.
It should be disabled during testing ( model.eval() ) since you may want to use full model (no element is masked)</p>
</blockquote>
<ol>
<li>argparse模块中的action参数</li>
</ol>
<p>用argparse模块让python脚本接收参数时，对于True/False类型的参数，向add_argument方法中加入参数action=‘store_true’/‘store_false’。
顾名思义，store_true就代表着一旦有这个参数，做出动作“将其值标为True”，也就是没有时，默认状态下其值为False。反之亦然，store_false也就是默认为True，一旦命令中有此参数，其值则变为False。</p>
<p>pytorch 中指定gpuid的两种方式</p>
<p>PyTorch 默认是使用从0 开始的GPU，如果 0正在使用，那么需要指定其他GPU。有三种方式进行指定：</p>
<ol>
<li>直接在终端设定</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-sh" data-lang="sh"><span class="nv">CUDA_VISIBLE_DEVICES</span> <span class="o">=</span><span class="m">1</span> python my_script.py
</code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>在python 代码中指定</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;CUDA_VISIBLE_DEVICES&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;0,1&#34;</span>
</code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>使用函数 set_device</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>官方建议使用前两种方式，即设置 <code>CUDA_VISIBLE_DEVICES</code></p>
<p>pytorch 中的 squeeze 函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># torch.Size([2, 1, 2, 1, 2])</span>

<span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># torch.Size([2, 2, 2])</span>

<span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 当给定dim时，那么挤压操作只在给定维度上</span>
<span class="k">print</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># torch.Size([2, 1, 2, 1, 2])</span>

<span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># torch.Size([2, 2, 1, 2])</span>

<span class="n">n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  <span class="c1"># torch.Size([2, 1, 2, 1, 2])</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>多维张量本质上就是一个变换，如果维度是 1 ，那么，1 仅仅起到扩充维度的作用，而没有其他用途，因而，在进行降维操作时，为了加快计算，是可以去掉这些 1 的维度。</p>
</blockquote>
<h3 id="pytorch-单机多卡操作总结分布式dataparallel混合精度horovodhttpsjishuinproginncomp763bfbd2f6b6"><a href="https://jishuin.proginn.com/p/763bfbd2f6b6">PyTorch 单机多卡操作总结：分布式DataParallel，混合精度，Horovod</a></h3>
<p>为了实现多GPU训练，我们必须想一个办法在多个GPU上分发数据和模型，并且协调训练过程。</p>
<p>单机多卡的方法：</p>
<p><strong>1、nn.DataParallel</strong> 简单方便的 nn.DataParallel</p>
<p><strong>2、torch.distributed</strong> 使用 torch.distributed 加速并行训练</p>
<p><strong>3、apex</strong> 使用 apex 再加速。</p>
<p>NVIDIA显卡机器型号</p>
<p>通常 NVIDIA 显卡的名字会是这样的一串 <code>RTX 2080 Super</code> 。第一串英文字母系列从低到高：GT GTX RTX 位端越高价格越高；第二串是当年推出的显卡，2019年 有 20 和16 两个系列， 2020年推出的是以 30 开头，后面 <code>80</code> 表示等级，数字越大等级越高；最后是后缀的英文，通常是 Ti 和 Super，其中 Ti 表示加强版，Super 表示升级版。</p>
<p>20年 NVIDIA （30 系列）</p>
<table>
<thead>
<tr>
<th>显卡型号</th>
<th>核心代号</th>
<th>制造工艺 (nm)</th>
<th>流处理器/RT核心/Tensor核心</th>
<th>核心频率 (MHz)</th>
<th>加速频率 (MHz)</th>
<th>显存位宽 (-bit)</th>
<th>显存容量</th>
<th>显存频率 (GHz)</th>
<th>整卡功耗 (W)</th>
</tr>
</thead>
<tbody>
<tr>
<td>RTX 3090</td>
<td>GA102-300</td>
<td>8nm</td>
<td>10496/82/328</td>
<td>1395</td>
<td>1695</td>
<td>384</td>
<td>24GB GDDR6X</td>
<td>19.5</td>
<td>350</td>
</tr>
<tr>
<td>RTX 3080 Ti</td>
<td>GA102-225</td>
<td>8nm</td>
<td>10240/80/320</td>
<td>1365</td>
<td>1665</td>
<td>384</td>
<td>12GB GDDR6X</td>
<td>19</td>
<td>350</td>
</tr>
<tr>
<td>RTX 3080</td>
<td>GA102-200/202</td>
<td>8nm</td>
<td>8704/68/272</td>
<td>1440</td>
<td>1710</td>
<td>320</td>
<td>10GB GDDR6X</td>
<td>19</td>
<td>320</td>
</tr>
<tr>
<td>RTX 3070 Ti</td>
<td>GA104-400</td>
<td>8nm</td>
<td>6144/48/96</td>
<td>1575</td>
<td>1770</td>
<td>256</td>
<td>8GB GDDR6X</td>
<td>19</td>
<td>290</td>
</tr>
<tr>
<td>RTX 3070</td>
<td>GA104-300/302</td>
<td>8nm</td>
<td>5888/46/184</td>
<td>1500</td>
<td>1725</td>
<td>256</td>
<td>8GB GDDR6</td>
<td>14</td>
<td>220</td>
</tr>
<tr>
<td>RTX 3060 Ti</td>
<td>GA104-200/202</td>
<td>8nm</td>
<td>4864/38/152</td>
<td>1410</td>
<td>1665</td>
<td>256</td>
<td>8GB GDDR6</td>
<td>14</td>
<td>200</td>
</tr>
<tr>
<td>RTX 3060</td>
<td>GA106-300/302</td>
<td>8nm</td>
<td>3584/28/112</td>
<td>1320</td>
<td>1777</td>
<td>192</td>
<td>12GB GDDR6</td>
<td>15</td>
<td>170</td>
</tr>
</tbody>
</table>
<blockquote>
<p>在这里，我直接贴出 Nvidia 家各主流GPU的一些参数。以及拥有 TensorCore 的GPU列表： 就目前而言，基本就只有V100 和 TITAN V 系列是支持 TensorCore 计算的。</p>
</blockquote>
<p><img src="http://123.56.8.10:8899/images/2021/07/01/v2-6c25b2fa8e27c437f5b5704bed1eb392_1440w.jpg" alt="img"></p>
<h3 id="分布式计算调研">分布式计算调研</h3>
<p><a href="https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html">https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html</a></p>
<blockquote>
<p>I like to implement my models in Pytorch because I find it has the best balance between control and ease of use of the major neural-net frameworks. Pytorch has two ways to split models and data across multiple GPUs: <a href="https://pytorch.org/docs/stable/nn.html#dataparallel"><code>nn.DataParallel</code></a> and <a href="https://pytorch.org/docs/stable/nn.html#distributeddataparallel"><code>nn.DistributedDataParallel</code></a>. <code>nn.DataParallel</code> is easier to use (just wrap the model and run your training script). However, because it uses one process to compute the model weights and then distribute them to each GPU during each batch, networking quickly becomes a bottle-neck and GPU utilization is often very low. Furthermore, <code>nn.DataParallel</code> requires that all the GPUs be on the same node and doesn’t work with <a href="https://nvidia.github.io/apex/amp.html">Apex</a> for <a href="https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/">mixed-precision</a> training.</p>
<p>自己基于 pytorch 实现 balance control and ease of use of the major neural-net</p>
<p>DP 的问题有两个：网络通信称为瓶颈；GPU 使用率很低。DP 要求所有 GPU都在same node</p>
</blockquote>
<blockquote>
<p>Multiprocessing with <code>DistributedDataParallel</code> duplicates the model across multiple GPUs, each of which is controlled by one process. (A process is an instance of python running on the computer; by having multiple processes running in parallel, we can take advantage of procressors with multiple CPU cores. If you want, you can have each process control multiple GPUs, but that should be obviously slower than having one GPU per process. It’s also possible to have multiple worker processes that fetch data for each GPU, but I’m going to leave that out for the sake of simplicity.) The GPUs can all be on the same node or spread across multiple nodes. (A node is one “computer,” including all of its CPUs and GPUs. If you’re using AWS, a node is one EC2 instance.) Every process does identical tasks, and each process communicates with all the others. Only gradients are passed between the processes/GPUs so that network communication is less of a bottleneck.</p>
<p>这个 example 没有实现每个进程中有多个 num_workers，一个进程对应的是一个 GPU。实现的是单个进程对应单个 num_worker</p>
<p>DDP 中只是传递 梯度，所以对于通信的要求低一些。</p>
</blockquote>
<blockquote>
<p>During training, each process loads its own minibatches from disk and passes them to its GPU. Each GPU does its own forward pass, and then the gradients are all-reduced across the GPUs. Gradients for each layer do not depend on previous layers, so the gradient all-reduce is calculated concurrently with the backwards pass to futher alleviate the networking bottleneck. At the end of the backwards pass, every node has the averaged gradients, ensuring that the model weights stay synchronized.</p>
<p>DDP 中前向是各自单独进行，后向传播是得到一个  averaged gradient。</p>
</blockquote>
<blockquote>
<p>All this requires that the multiple processes, possibly on multiple nodes, are synchronized and communicate. Pytorch does this through its <a href="https://pytorch.org/docs/stable/distributed.html#initialization"><code>distributed.init_process_group</code></a> function. This function needs to know where to find process 0 so that all the processes can sync up and the total number of processes to expect. Each individual process also needs to know the total number of processes as well as its rank within the processes and which GPU to use. It’s common to call the total number of processes the <em>world size</em>. Finally, each process needs to know which slice of the data to work on so that the batches are non-overlapping. Pytorch provides <a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html"><code>nn.utils.data.DistributedSampler</code></a> to accomplish this.</p>
<p>pytorch 中使用 <a href="https://pytorch.org/docs/stable/distributed.html#initialization"><code>distributed.init_process_group</code></a>  来控制进程之间的通信； 使用  <a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html"><code>nn.utils.data.DistributedSampler</code></a>  来控制数据的分配。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Initialize the process and join up with the other processes. This is “blocking,” meaning that no process will continue until all processes have joined. I’m using the <code>nccl</code> backend here because the <a href="https://pytorch.org/docs/stable/distributed.html">pytorch docs</a> say it’s the fastest of the available ones. The <code>init_method</code> tells the process group where to look for some settings. In this case, it’s looking at environment variables for the <code>MASTER_ADDR</code> and <code>MASTER_PORT</code>, which we set within <code>main</code>. I could have set the <code>world_size</code> there as well as <code>WORLD_SIZE</code>, but I’m choosing to set it here as a keyword argument, along with the global rank of the current process.</p>
<p>进程启动和通信</p>
</blockquote>
<blockquote>
<p>如果是多 node，那么需要使用多个 terminal 一块进行启动。目前来说我只是在单个node 上进行实验。</p>
</blockquote>
<p>To run this on, say, 4 nodes with 8 GPUs each, we need 4 terminals (one on each node). On node 0 (as set by line 13 in <code>main</code>):</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">python src/mnist-distributed.py -n 4 -g 8 -nr 0
</code></pre></td></tr></table>
</div>
</div><p>Then, on the other nodes:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">python src/mnist-distributed.py -n 4 -g 8 -nr i
</code></pre></td></tr></table>
</div>
</div><p>pytorch 提供两种方法在多 GPU 上切分数据和模型</p>
<ul>
<li>dataParallel</li>
<li>distributedataparallel</li>
</ul>
<p>DataParallel更易于使用。不过，通信是瓶颈，GPU利用率通常很低，而且不支持分布式。DistributedDataParallel支持模型并行和多进程，单机/多机都可以，是分布训练。</p>
<p><strong>混合精度（mixed precision）知识点</strong></p>
<p>默认深度学习模型训练过程中使用的都是 fp32（pytorch 中 float 就是 fp32， python 中 float 是 fp64）。apex 是NVIDIA 的一个pytorch 扩展，用于支持混合精度训练和分布式训练。自 pytorch1.6 开始，已经内置了 torch.cuda.amp，采用自动混合精度训练不再需要加载第三方 NVIDIA 的apex 库。</p>
<p>fp16 半精度</p>
<p>半精浮点数是一种计算机使用的二级制浮点数数据类型，使用 2 字节（16位）存储。</p>
<p><img src="http://123.56.8.10:8899/images/2021/05/31/1781642-20210122192609433-1423562086.png" alt="img"></p>
<p>为什么要使用 fp16</p>
<ul>
<li>减少显存占用。由于 FP16 的内存占用只有 FP32 的一半，所以可以节省一半的显存空间。</li>
<li>加快训练和推理的计算。除了能减少内存的使用，还能节省模型训练时间，在大部分的测试中，基于 FP16 的加速方法，能够带来多一倍的加速体验。</li>
<li>张量核心的普及（NVIDIA tensor core），低精度计算是未来深度学习的一个重要趋势。</li>
</ul>
<p>但是，FP16 带来以下两个问题：</p>
<ul>
<li>溢出错误：由于 FP16 的动态范围比 FP32 狭窄很多，所以在计算中很容易出现上溢出和下溢出，溢出之后就会出现 <code>nan</code> 问题。在深度学习中，由于激活函数的梯度往往比权重梯度小，所以更加容易出现下溢出问题。</li>
<li>舍入误差：当梯度小鱼当前区间内的最小间隔，那么这次梯度更新就可能失败。</li>
</ul>
<p>解决问题的办法：混合精度训练 + 动态损失放大</p>
<ul>
<li>混合精度训练（mixed precision）：在内存中使用 FP16 做存储和乘法从而加速计算，使用 FP32 做累加避免舍入误差。</li>
</ul>
<p>在内存中用 FP16作存储和乘法从而加速计算，用 fp32 作累加避免舍入误差。混合精度训练的策略有效避免了舍入误差。在pytorch1.6的AMP上下文中，以下操作中Tensor会被自动转化为半精度浮点型torch.HalfTensor：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">__matmul__
addbmm
addmm
addmv
addr
baddbmm
bmm
chain_matmul
conv1d
conv2d
conv3d
conv_transpose1d
conv_transpose2d
conv_transpose3d
linear
matmul
mm
mv
prelu
</code></pre></td></tr></table>
</div>
</div><ul>
<li>损失放大 （loss scaling）：即使使用了混合精度训练，还是会存在无法收敛的情况，原因是激活梯度的值太小，造成了下溢出（underflow）。损失放大的思路：
<ul>
<li>反向传播前，将损失变化（dLoss）手动增大 $2^k$倍，因此反向传播时得到的中间变量（激活函数梯度)则不会溢出；</li>
<li>反向传播后，将权重梯度缩$2^k$ 倍，恢复正常值。</li>
</ul>
</li>
</ul>
<p>AMP（automatic mixed precision）自动混合精度包括：</p>
<ul>
<li>自动： tensor 的 dtype 类型可以自动变化，框架按需自动调整 tensor的 dtype</li>
<li>混合精度：包括 torch.FloatTensor 和 torch.HalfTensor</li>
</ul>
<p>pytorch 1.6 的新包： torch.cuda.amp, 是 NVIDIA 开发人员贡献到 pytorch 中的，只有支持 tensor core 的 cuda 硬件才能享受 AMP 带来的优势。 tensor core 是一种矩阵乘累加的计算单元，每个 tensor core 时针执行 64 个浮点混合精度操作（fp16 矩阵相乘 和 fp32 累加）</p>
<p>如果 pytorch1.5 之前的版本，可以使用 NVIDIA 的三方包 <code>apex.amp</code> ,下面给出了一个样例。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">apex</span> <span class="kn">import</span> <span class="n">amp</span>
<span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">amp</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">opt_level</span><span class="o">=</span><span class="s2">&#34;O1&#34;</span><span class="p">)</span> <span class="c1"># 这里是“欧一”，不是“零一”</span>
<span class="k">with</span> <span class="n">amp</span><span class="o">.</span><span class="n">scale_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span> <span class="k">as</span> <span class="n">scaled_loss</span><span class="p">:</span>
    <span class="n">scaled_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>这个方式相对来说是更加容易的。</p>
</blockquote>
<p>其中只有一个<code>opt_level</code>需要用户自行配置：</p>
<ul>
<li><code>O0</code>：纯FP32训练，可以作为accuracy的baseline；</li>
<li><code>O1</code>：混合精度训练（推荐使用），根据黑白名单自动决定使用FP16（GEMM, 卷积）还是FP32（Softmax）进行计算。</li>
<li><code>O2</code>：“几乎FP16”混合精度训练，不存在黑白名单，除了Batch norm，几乎都是用FP16计算。</li>
<li><code>O3</code>：纯FP16训练，很不稳定，但是可以作为speed的baseline；</li>
</ul>
<p>安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">git clone https://github.com/NVIDIA/apex
<span class="nb">cd</span>  apex
</code></pre></td></tr></table>
</div>
</div><p>如果是 pytorch 1.6 及其以上的版本，那么可以使用自带的接口： <code>autocast</code> 和 <code>Gradscaler</code></p>
<p>目前 <code>autocast</code> 已经调通，使用 AMP 之后的结果，训练时候的显存占用从原先的 11G 下降到了 6 GB。</p>
<blockquote>
<p>所以这个 AMP 还是很厉害的一项优化技术。</p>
</blockquote>
<p>还不知道 <code>Gradscaler</code> 如何进行使用。</p>
<blockquote>
<p>好像只有等到 使用 autocast 报错之后，然后才需要使用 gradscaler 去调整</p>
</blockquote>
<p>方法一： 使用 dataparallel</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># main.py</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>

<span class="n">gpus</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s1">&#39;cuda:{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="o">...</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=...</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">device_ids</span><span class="o">=</span><span class="n">gpus</span><span class="p">,</span> <span class="n">output_device</span><span class="o">=</span><span class="n">gpus</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
   <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
      <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="o">...</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
      <span class="o">...</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>DataParallel 会自动帮我们将数据切分 load 到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总</p>
</blockquote>
<p>缺点：</p>
<p>单进程计算模型的权重，然后分发到其他的 GPU上，所以如果是多机器多卡，那么网络通信就成为了一个瓶颈。所以当使用 <code>nn.DataParallel</code> 时候，尽量保证是单机器多卡。（GPU负载不均衡，通常是第一个GPU 的使用率更高一些）
另外这种方式不支持混合精度训练。</p>
<p>方法二：使用 <code>torch.distributed 加速并行训练</code></p>
<p>DataParallel：单进程控制多 GPU。</p>
<p>DistributedDataParallel：多进程控制多 GPU。</p>
<p>和单进程训练不同的是，多进程训练需要注意以下事项：</p>
<ul>
<li>在喂数据的时候，一个batch被分到了好几个进程，每个进程在取数据的时候要确保拿到的是不同的数据（<code>DistributedSampler</code>）；</li>
<li>要告诉每个进程自己是谁，使用哪块GPU（<code>args.local_rank</code>）；</li>
<li>在做BatchNormalization的时候要注意同步数据。</li>
</ul>
<p>同步BN</p>
<blockquote>
<p>现有的标准 Batch Normalization 因为使用数据并行（Data Parallel），是单卡的实现模式，只对单个卡上对样本进行归一化，相当于减小了批量大小（batch-size）（详见BN工作原理部分）。对于比较消耗显存的训练任务时，往往单卡上的相对批量过小，影响模型的收敛效果。之前在我们在图像语义分割的实验中，Jerry和我就发现使用大模型的效果反而变差，实际上就是BN在作怪。跨卡同步 Batch Normalization 可以使用全局的样本进行归一化，这样相当于‘增大‘了批量大小，这样训练效果不再受到使用 GPU 数量的影响。最近在图像分割、物体检测的论文中，使用跨卡BN也会显著地提高实验效果，所以跨卡 BN 已然成为竞赛刷分、发论文的必备神器。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># main.py</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--local_rank&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;node rank for distributed training&#39;</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">#每个进程一个sampler</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=...</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">])</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
   <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
      <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="o">...</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
      <span class="o">...</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>3 使用 apex （混合精度）训练</p>
<blockquote>
<p>Apex 是 NVIDIA 开源的用于混合精度训练和分布式训练库。Apex 对混合精度训练的过程进行了封装，改两三行配置就可以进行混合精度的训练，从而大幅度降低显存占用，节约运算时间。此外，Apex 也提供了对分布式训练的封装，针对 NVIDIA 的 NCCL 通信库进行了优化。</p>
</blockquote>
<p>（这个是需要硬件支持的，目前只有 Tesla V100 和 TTITAN V 系列支持）</p>
<p>判断GPU 是否支持 FP16， 支持 tensor core 的GPU （2080 ti， titan， tesla），不支持的（Pascal 系列）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># main.py</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>

<span class="kn">from</span> <span class="nn">apex.parallel</span> <span class="kn">import</span> <span class="n">convert_syncbn_model</span>
<span class="kn">from</span> <span class="nn">apex.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--local_rank&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s1">&#39;node rank for distributed training&#39;</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=...</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="c1">#同步BN</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">convert_syncbn_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="c1">#混合精度</span>
<span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">amp</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="c1">#分布数据并行</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">])</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
   <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
      <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="o">...</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="k">with</span> <span class="n">amp</span><span class="o">.</span><span class="n">scale_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span> <span class="k">as</span> <span class="n">scaled_loss</span><span class="p">:</span>
         <span class="n">scaled_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span>  <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&#34;data&#34;</span><span class="p">,</span> <span class="n">train</span> <span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span><span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>如果只是做测试，那么 transformers.ToTensor() 就足够了</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span> <span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span> <span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">pin_memory</span> <span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>pin_memory 是锁页内存，创建 DataLoader 时候，设置 <code>pin_memory=True</code>,  这样将内存中的 tensor转义到 GPU显存中更快一些。</p>
<blockquote>
<p>主机中的内存有两种存在方式，一种是锁页，一种是不锁页，锁页内存存放的内容在任何情况下都不会和主机中的虚拟内存（虚拟内存就是硬盘）进行交换。而不锁页内存在主机内存不足的情况下，数据会存放到虚拟内存中。</p>
<p>显卡中的显存全部都是锁页内存。</p>
</blockquote>
<p>所以在内存比较充足的情况下，建议加上这个参数，这样可以加快速度。</p>
<p>参考文献</p>
<p><a href="https://www.jianshu.com/p/9779683ffa58">介绍PyTorch的简单示例</a>
<a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#examples">LEARNING PYTORCH WITH EXAMPLES</a></p>
<p><a href="https://www.cnblogs.com/jimchen1218/p/14315008.html">Pytorch自动混合精度(AMP)介绍与使用</a></p>
<p>Horovod</p>
<blockquote>
<p>Uber 开源的分布式深度学习框架, 最初是面向 tensorflow 的分布式训练框架, 现在也支持pytorch 等多个深度学习框架. 所以这个是跨平台的分布式训练工具, 兼容 tensorflow, keras 和 pytorch.</p>
</blockquote>
<p>按照并行方法, 分布式训来年一般分为数据并行和模型并行两种:</p>
<ul>
<li>模型并行, 分布式中的不同GPU 负责网络模型的不同部分. 例如,神经网络模型的不同网络层被分配到不同的 GPU, 或者同一层内部的不同参数被分配到不同 GPU</li>
<li>数据并行, 不同的GPU 有同一个模型的多个副本, 每个GPU 分配到不同的数据, 然后将所有 GPU 的计算按照哦阿某种方式合并.</li>
</ul>
<p>注意,上述中的不同 GPU 可以是同一个机器上的多个 GPU, 也可以是不同机器上的GPU.</p>
<p>当然也有数据并行和模型并行的混合模式.  模型并行各个部分存在一定的依赖, 在实际训练中用得不多. 而数据并行,各个部分独立,在实际中更加常用, 提速效果也更好.</p>
<p>数据并行会涉及到各个 GPU 之间同步模型参数, 一般分为同步更新和异步更新. 同步更新需要等到所有 GPU 的梯度计算完成, 再统一计算新权重, 然后所有 GPU 同步数值后, 才进行下一轮的计算. 异步更新, 每个 gpu 梯度计算完后, 无需等待其他 gpu 的梯度计算, 即可进行下一轮的计算.  同步更新有等待, 异步更新基本上没有等待, 但异步更新涉及到梯度过时等更复杂问题.</p>
<p>无论是单机多卡, 还是多机多卡, 都是分布式运算.</p>
<p>使用</p>
<p>在单机4 卡上进行训练</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">horovodrun -np <span class="m">4</span> -H localhost:4 python train.py
</code></pre></td></tr></table>
</div>
</div><p>在四机器,每个机器 4卡上进行训练, 只需要在一个机器上执行命令</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">horovodrun -np <span class="m">16</span> -H server1:4,server2:4,server3:4,server4:4 python train.py
</code></pre></td></tr></table>
</div>
</div><p>horovod 只支持同步更新的数据并行. 模型并行和异步更新的数据并行, 应该是不支持的.</p>
<p>显存占用 85% 左右就行, 不要占满, 偶尔有些操作会上蹿, 一点余量都不给容易 OOM. memory-usage 不是关键, 关键是 GPU-util 拉满才是王道, 毕竟后者才是实打实的计算.</p>
<p>对比</p>
<p><img src="http://123.56.8.10:8899/images/2021/08/03/image-20210803160343127.png" alt="image-20210803160343127" style="zoom:67%;" /></p>
<blockquote>
<p>Horovod 安装有点麻烦. DDP 一时爽,迁移火葬场.</p>
</blockquote>
<p>由于 pytorch 已经实现了分布式训练,  所以可以不选择 horovod.</p>
<p>在学习的时候, 可以参考官方的 readme 和相应的 github repo, 比较好的项目, 一般都是有 <code>examples</code> 这样的东西.</p>
<p><a href="https://github.com/horovod/horovod/tree/master/examples">https://github.com/horovod/horovod/tree/master/examples</a></p>
<blockquote>
<p>可以进一步看看这个.</p>
</blockquote>
<h3 id="拷贝">拷贝</h3>
<p>（1）tensor.clone()</p>
<p>返回一个完全一样的 tensor，新的 tensor 开辟新的内存，但是仍保留在计算图中。</p>
<p>clone 操作在不共享数据内存的同时，支持梯度回溯，所以常用在神经网络中某个单元需要重复使用的场景下。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-shell" data-lang="shell">import torch
<span class="nv">a</span> <span class="o">=</span>torch.tensor<span class="o">(</span>1.0, <span class="nv">requires_grad</span> <span class="o">=</span>True<span class="o">)</span>
<span class="nv">b</span> <span class="o">=</span> a.clone<span class="o">()</span>
id<span class="o">(</span>a<span class="o">)</span>, id<span class="o">(</span>b<span class="o">)</span> <span class="c1"># 判断 a和 b 是不是一个对象</span>
a.data_ptr<span class="o">()</span>, b.data_ptr<span class="o">()</span> <span class="c1"># 判断 a b 是否指向同一块内存</span>

</code></pre></td></tr></table>
</div>
</div><p>（2） tensor.detach()</p>
<p>从计算图中脱离</p>
<blockquote>
<p>返回的新的 tensor 和原来的 tensor 共享数据内存，但不涉及计算，即 <code>requires_grad =False</code>。因为是共享同一内存，对其中一个tensor 执行原地操作（比如 <code>resize_</code>,  <code>set_</code>） 则会引发错误。</p>
</blockquote>
<p>detach 操作在共享内存，脱离计算图，所以常用在神经网络中仅需要利用张量数值，而不需要追踪梯度的场景下。</p>
<p>（3）tensor.clone().detach() 和 tensor.detach().clone()</p>
<p>两者的结果是一样的，即返回的 tensor 和原 tensor 在梯度上和数据上都没有任何关系，一般使用前者。</p>
<p>（4）<code>copy_()</code></p>
<p><code>Tensor.``copy_</code>(<em>src</em>, <em>non_blocking=False</em>) → Tensor</p>
<p>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</p>
<p>The <code>src</code> tensor must be <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the <code>self</code> tensor. It may be of a different data type or reside on a different device.</p>
<ul>
<li>
<p>Parameters</p>
<p><strong>src</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a>) – the source tensor to copy from<strong>non_blocking</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a>) – if <code>True</code> and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.</p>
</li>
</ul>
<blockquote>
<p>inplace 的操作</p>
<p>支持 broadcast</p>
</blockquote>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">jijeng</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2019-02-01
        
    </span>
  </p>
  
  
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="http://47.94.35.231:9998/blog_imgs/wechatpay.png">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="http://47.94.35.231:9998/blog_imgs/alipay.png">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/pytorch/">pytorch</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/algorithm_practice_1/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Algorithm Practice (1)</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/model_training/">
            <span class="next-text nav-default">Model Training</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="http://jijeng.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2018 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span>jijeng</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
