<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>CNN Understanding - Jijeng&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="jijeng" /><meta name="description" content="CNN 网络中常见层的介绍和理解。
" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.79.1 with theme even" />


<link rel="canonical" href="http://jijeng.github.io/post/cnn_understanding/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="CNN Understanding" />
<meta property="og:description" content="CNN 网络中常见层的介绍和理解。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://jijeng.github.io/post/cnn_understanding/" />
<meta property="article:published_time" content="2020-04-07T10:41:47+08:00" />
<meta property="article:modified_time" content="2020-04-07T10:41:47+08:00" />
<meta itemprop="name" content="CNN Understanding">
<meta itemprop="description" content="CNN 网络中常见层的介绍和理解。">
<meta itemprop="datePublished" content="2020-04-07T10:41:47+08:00" />
<meta itemprop="dateModified" content="2020-04-07T10:41:47+08:00" />
<meta itemprop="wordCount" content="6279">



<meta itemprop="keywords" content="cnn," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="CNN Understanding"/>
<meta name="twitter:description" content="CNN 网络中常见层的介绍和理解。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Jijeng&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Jijeng&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">CNN Understanding</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-04-07 </span>
        <div class="post-category">
            <a href="/categories/deep-learning/"> deep learning </a>
            </div>
          <span class="more-meta"> 约 6279 字 </span>
          <span class="more-meta"> 预计阅读 13 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#五种卷积">五种卷积</a>
          <ul>
            <li><a href="#separable-convolution">Separable Convolution</a></li>
            <li><a href="#group-convolution">Group Convolution</a></li>
            <li><a href="#heading"></a></li>
            <li><a href="#dilation-convolution">Dilation Convolution</a></li>
            <li><a href="#transposed-convolutions">Transposed Convolutions</a></li>
            <li><a href="#deformable-convolutional-networks">Deformable convolutional networks</a></li>
            <li><a href="#sparse-convolution">sparse convolution</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>CNN 网络中常见层的介绍和理解。</p>
<p>一、边界检测（引子）</p>
<p>通过设计某个滤波器（filter，也可以称为 kernel），去和图像做卷积操作，可以发现图像中的某些特征。CNN 就是通过一个个 filter， 不断提取特征，从局部的特征到整体的特征，从而完成了图像识别的功能。</p>
<p>二、CNN 中的基本概念</p>
<p>CNN网络中常见的结构是：卷积、池化和激活。卷积层是CNN 网络的核心，激活函数获得非线性特征，池化层主要体现在降采样：保留显著特征、降低特征的维度、扩大 kernel 的感受野。</p>
<p>pooling 可以提供旋转不变性?</p>
<blockquote>
<p>更加准确的是说 max pooling 具有微小形变的鲁棒性。当我们正面拍一些图像，在某些地方得到 activation，然后旋转一定的角度，依然能够在这个点得到 activation。这个其实 maxpooling 的决定的。</p>
</blockquote>
<ol>
<li>padding 填白</li>
</ol>
<p>从上面的引子中，我们可以知道，原图像在经过filter卷积之后，变小了，从(8,8)变成了(6,6)。假设我们再卷一次，那大小就变成了(4,4)了。这样有啥问题呢？ 主要有两个问题：</p>
<ul>
<li>每次卷积，图像都缩小，这样卷不了几次就没了；</li>
<li>相比于图片中间的点，图片边缘的点在卷积中被计算的次数很少。这样的话，边缘的信息就易于丢失。</li>
</ul>
<p>为了解决这个问题，我们可以采用padding的方法。我们每次卷积前，先给图片周围都补一圈空白，让卷积之后图片跟原来一样大，同时，原来的边缘也被计算了更多次。</p>
<p>我们把上面这种“让卷积之后的大小不变”的padding方式，称为 “Same”方式， 把不经过任何填白的，称为 “Valid”方式。这个是我们在使用一些框架的时候，需要设置的超参数。</p>
<ol start="2">
<li>stride 步长</li>
</ol>
<p>前面我们所介绍的卷积，都是默认步长是1，但实际上，我们可以设置步长为其他的值。</p>
<p>3.pooling 池化</p>
<p>这个pooling，是为了提取一定区域的主要特征，并减少参数数量，防止模型过拟合。 除了MaxPooling,还有AveragePooling，顾名思义就是取那个区域的平均值。</p>
<p>global average pooling or global max pooling</p>
<p>GAP 可以实现任意图像大小的输入。</p>
<p>global average pooling 和 average pooling （local）的差别在于 global，local 是取feature map 上的一个子区域求均值和最大值，然后滑动子区域；global 就是针对整个 feature map 求解均值和最大值。</p>
<p>Global Pooling就是<strong>池化窗口的大小 = 整张特征图的大小</strong>。这样，每个 W×H×C 的特征图输入就会被转化为 1×1×C 的输出，也等同于每个位置权重都为 1/(W×H) 的全连接层操作。</p>
<p>使用全局池化之后，特征图每个 channel 都被压缩成了一个点，实际上是对没给个channel 进行了信息压缩。</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/25/1576078449-7190--700x298.jpg" alt="关于global average pooling理解和介绍"></p>
<p>4.对多通道（channels）图片的卷积</p>
<p>如果输入图片是三维的呢（即增多了一个channels），比如是(8,8,3)，这个时候，我们的filter的维度就要变成(3,3,3)了，它的 最后一维要跟输入的channel维度一致。 这个时候的卷积，是三个channel的所有元素对应相乘后求和，也就是之前是9个乘积的和，现在是27个乘积的和。因此，输出的维度并不会变化。还是(6,6)。</p>
<p>但是，一般情况下，我们会 使用多了filters同时卷积，比如，如果我们同时使用4个filter的话，那么 输出的维度则会变为(6,6,4)。</p>
<p>三、CNN的结构组成</p>
<ol>
<li>
<p>Convolutional layer（卷积层&ndash;CONV）</p>
</li>
<li>
<p>Pooling layer （池化层&ndash;POOL）</p>
</li>
</ol>
<blockquote>
<p>Pooling layers are generally used to reduce the size of the inputs and hence speed up the computation.</p>
</blockquote>
<img src="http://123.56.8.10:8899/images/2021/03/25/6ngw4u.png" width="80%" height="80%">
<p>注意这里的 filter size 是2， 然后stride 是2.</p>
<p>In summary, the hyperparameters for a pooling layer are:</p>
<ul>
<li>Filter size</li>
<li>Stride</li>
<li>Max or average pooling</li>
</ul>
<p>CNN 中 stride、kernel、padding 的计算</p>
<p>定义以下参数定义 <img src="https://www.zhihu.com/equation?tex=stride++%3D+S" alt="[公式]">,  <img src="https://www.zhihu.com/equation?tex=kernelsize+%3D+F%28kernel++size+%3D+F+%2AF%29+" alt="[公式]">, <img src="https://www.zhihu.com/equation?tex=padding+%3D+P+" alt="[公式]"></p>
<p>输入尺寸为 <img src="https://www.zhihu.com/equation?tex=W" alt="[公式]"></p>
<p>输出尺寸为 <img src="https://www.zhihu.com/equation?tex=W_%7Bnew%7D" alt="[公式]"></p>
<p>则有卷积尺寸变化为
$$
W_{n e w}=\frac{W-F+2 * P}{S}+1
$$</p>
<p>比如，输入是28，卷积核是$3*3$, 步长 stride为 1，padding 为1，通过上述的公式得到新的输出是 28。和原来的输入是一样的。</p>
<ol start="3">
<li>Fully Connected layer（全连接层&ndash;FC）</li>
</ol>
<img src="http://123.56.8.10:8899/images/2021/03/25/GRcq81.png" width="80%" height="80%">
<p>这里需要说明的是，在经过数次卷积和池化之后，我们 最后会先将多维的数据进行“扁平化”，也就是把 (height,width,channel)的数据压缩成长度为 height × width × channel 的一维数组，然后再与 FC层连接，这之后就跟普通的神经网络无异了。</p>
<ol start="4">
<li>Batch normalization &amp; ReLU</li>
</ol>
<blockquote>
<p>After applying filters on the input, we apply a batch normalization followed by a ReLU for non-linearity. The batch normalization renormalizes data to make learning faster with the Gradient descent.
使用batch normalization是为了 learn faster（gradient descent）； 使用 relu 是为了增加非线性</p>
</blockquote>
<p>全连接层过多的参数会导致过拟合，所以有一些专门的方法去解决过拟合，比如说 dropout。</p>
<p>四、卷积网络 VS. 传统神经网络</p>
<p>卷积网络相比于传统神经网络有两点好处：</p>
<p>1.参数共享机制（parameters sharing）</p>
<p>参数共享机制，让我们的网络的参数数量大大地减少。<a href="https://zhuanlan.zhihu.com/p/42559190">从此明白了卷积神经网络（CNN）</a></p>
<ol start="2">
<li>连接的稀疏性（sparsity of connections）</li>
</ol>
<p>由卷积的操作可知，输出图像中的任何一个单元，只跟输入图像的一部分有关系：</p>
<p>Computer Vision 中常见的是以下三类问题：</p>
<ul>
<li>Image classification</li>
<li>Object detection</li>
<li>Neural style transfer</li>
</ul>
<p>1D convolution is covered here, because it’s usually under-explained, but it has noteworthy benefits.</p>
<blockquote>
<p>They are used to reduce the depth (number of channels). Width and height are unchanged in this case. If you want to reduce the horizontal dimensions, you would use pooling, increase the stride of the convolution, or don’t add paddings. The 1D convolutions computes a weighted sum of input channels or features, which allow selecting certain combinations of features that are useful downstream.
可以降低 number of channels （depth），而不改变 width or height。如果想要改变该尺寸，那么使用 pooling 技术</p>
</blockquote>
<p>CNN 概念介绍</p>
<p>alexnet</p>
<p><img src="http://47.94.35.231:9998/blog_imgs/image-20201119163445214.png" alt="image-20201119163445214" style="zoom:50%;" /></p>
<p>卷积简单说是相乘然后累加求和。</p>
<p>卷积填充中的mode： valid，same，full。valid：不进行填充；same：是以卷积核的中心和图像array的左上角进行对齐；full：以卷积核的右下角和图像array的左上角对齐。</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/25/image-20201119163722313.png" alt="image-20201119163722313" style="zoom:50%;" /></p>
<p><strong>卷积核</strong></p>
<p>水平卷积核识别横线</p>
<p>垂直卷积核识别竖线</p>
<p>Scharr边界更加锐利</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/25/image-20201119171813112.png" alt="image-20201119171813112" style="zoom:50%;" /></p>
<p>所以卷积核是否能够识别，归结为“物以类聚”。如果卷积核形状和图像的物体形状相似，那么是容易识别；如果相反，那么不容易识别。</p>
<p>传统的CNN 结构</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/25/image-20201119173233995.png" alt="image-20201119173233995" style="zoom:50%;" /></p>
<p>同一图像输入，用不同的卷积核得到的是不同的结果，但其中也有一些类似的地方。</p>
<p>为什么深层次的网络识别效果好？</p>
<ul>
<li>深层次网络的 receptive （感受野）更大</li>
<li>浅层次的特征可以汇聚成更复杂的特征</li>
</ul>
<p>pooling: 降采样的操作，提高效率（特征数量减少）；使得某些特征更健壮。</p>
<p>dropout 中的 <code>keep_prob=1</code> 表示保留全部的数据； <code>keep_prob=0</code> 表示全部丢弃数据。注意其余的数据还需要进行<code>1/keep_prob</code> ，以保证和不变。</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/25/image-20201127164513099.png" alt="image-20201127164513099" style="zoom:70%;" /></p>
<p>在卷积网络中一般倾向于使用多个小的卷积核而不是一个大的卷积核。原因：参数量 $7<em>7 &gt; 3</em>3*3$；多层卷积核可以引入非线性。</p>
<p>adam 对于不同的参数使用不同的学习率；BGD 对于不同的参数使用相同的学习率。所以建议使用 adam 作为optim。</p>
<p>图像识别的实质仍然是分类，进一步的工作就是目标检测。（如果图像分类做好了，那么是可以去做目标检测）。object detection的演变流程：</p>
<ul>
<li>R-CNN： region-CNN， 首先是对于图像处理得到若干CNN 能够直接处理的小图，然后每个图进行得到的embedding +svm 得到该小框物体的置信度，执行度最大的话就是最后的结果。</li>
<li>Fast R-CNN：首先图像过一个CNN得到feature map，然后提取region 中相应的feature，feature +svm 得到置信度。这样共享了 feature map，大大提高了时间效率。（解决的是多次计算CNN 的效率问题）</li>
<li>Faster R-CNN：首先图像过CNN得到feature map，然后通过<code>region proposel network</code> 从CNN 中得到框，然后通过一个 classifer得到置信度。（解决的是计算 region的效率问题）</li>
</ul>
<h2 id="五种卷积">五种卷积</h2>
<p>训练更加高效的神经网络逐渐成为人们关注的焦点，设计轻量级模型主要通过以下三种方式：</p>
<p>（1）减少网络参数量和计算量</p>
<p>（2）量化参数，减少每个参数占用内存</p>
<p>（3）神经网络架构搜索（NAS）</p>
<p>以下四种网络是通过第一种方式来设计轻量级模型。</p>
<h3 id="separable-convolution">Separable Convolution</h3>
<p>Separable Convolution 或者 Depthwise Separable Convolution 来自Google设计的移动端轻量化网络 mobilenet。</p>
<p>Separable Convolution 将传统卷积分解为Depthwise Convolution与Pointwise Convolution两部分，有效的减小了参数数量。</p>
<p>下面给出一个例子说明。</p>
<p>此时，卷积层共4个Filter，每个Filter包含了3个Kernel，每个Kernel的大小为3×3。因此卷积层的参数数量可以用如下公式来计算：</p>
<p>核心思想是将一个完整的卷积运算分解为两步进行，Depthwise Convolution（逐通道卷积，卷积核的shape $ 卷积核w * 卷积核 h * 输入通道数$）与Pointwise Convolution（逐点卷积，卷积核的shape $1*1 * 输入通道数 * 输出通道数$）。</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/31/image-20210331140611578.png" alt="image-20210331140611578" style="zoom:50%;" /></p>
<p>Depthwise Convolution完成后的Feature map数量与输入层的depth相同，但是这种运算对输入层的每个channel独立进行卷积运算后就结束了，没有有效的利用不同map在相同空间位置上的信息。因此需要增加另外一步操作来将这些map进行组合生成新的Feature map，即接下来的Pointwise Convolution。</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/31/image-20210331140531938.png" alt="image-20210331140531938" style="zoom:50%;" /></p>
<p>Pointwise Convolution的运算与常规卷积运算非常相似，不同之处在于卷积核的尺寸为 1×1×M，M为上一层的depth。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。</p>
<p>对比一下两个的参数量：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 传统常规的卷积方式
N_std = 4 × 3 × 3 × 3 = 108
# 深度可分离卷积
N_depthwise = 3 × 3 × 3 = 27
N_pointwise = 1 × 1 × 3 × 4 = 12
N_separable = N_depthwise + N_pointwise = 39
</code></pre></td></tr></table>
</div>
</div><p>所以同样是 4 个feature map， separable convolution 的参数个数基本上是常规卷积的 $1/3$。所以在参数量相同的情况下，采用 Separable Convolution 的神经网络可以做到更深。</p>
<p>普通卷积同时考虑区域和通道，而可分离卷积先考虑区域然后再考虑通道，实现了区域和通道的分离。</p>
<p>但 Separable Convolution 也不是没有缺点，使用该卷积的网络结构在 train 的时候占用显存比较大。</p>
<blockquote>
<p>efficientnet 的模型很小，因为本地保存的模型 ckpt 容量代销和模型的参数量相关，模型参数和模型自身 layer 的设计有关，但是模型训练时占用的显存不是和模型参数量成一定的线性关系，更多的是和模型 inference 与 backward 过程中产生的 feature map 大小有关，另外和网络的深度也有一定的关系（这样中间变量可能更多了），在 backward 的时候更是直接翻倍，因为需要保存特征图的梯度，可能需要 2 倍的显存占用。</p>
<p>同时网络中的多分支也是比较消耗内存的，参考凯明大佬的regnet</p>
</blockquote>
<p><img src="http://123.56.8.10:8899/images/2021/03/31/v2-7ff329bfba3195dcb094651605ecd5eb_1440w8c7f344b7be86c34.jpg" alt="img" style="zoom:50%;" /></p>
<p>参考文献</p>
<p>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</p>
<p><a href="https://zhuanlan.zhihu.com/p/80041030">Depthwise卷积与Pointwise卷积</a></p>
<p><a href="https://yinguobing.com/separable-convolution/">卷积神经网络中的Separable Convolution</a></p>
<p><a href="https://www.zhihu.com/question/388773309">EfficientDet和EfficientNet的模型很小，为什么占用的显存很高呢?</a></p>
<h3 id="group-convolution">Group Convolution</h3>
<p>分组卷积最早出现在 AlexNet 中，当时的硬件资源有限，训练 AlexNet 时卷积操作不能全部放在同一个 GPU 处理，所以作者把 feature maps 分给多个 GPU 分别进行处理，最后把多个 GPU 的结果进行融合。</p>
<p><img src="http://123.56.8.10:8899/images/2021/11/22/image-20211122105541483.png" alt="image-20211122105541483"></p>
<p>图中将输入数据分成 2 组（组数为 g），注意这个只是在深度上进行划分，即几个通道编成一组，具体的数量由 $C_1 /g$ 决定。因为输出数据的改变，相应的，卷积核也需要做出同样的改变。即每组中卷积核的深度也就变成了（C1/g），而卷积核的大小是不需要改变的，此时每组的卷积核的个数就变成了（C2/g）个，而不是原来的C2了。</p>
<p>然后用每组的卷积核同它们对应组内的输入数据卷积，得到了输出数据以后，再用concatenate的方式组合起来，最终的输出数据的通道仍旧是C2。也就是说，分组数g决定以后，那么我们将并行的运算g个相同的卷积过程，每个过程里（每组），输入数据为H1×W1×C1/g，卷积核大小为h1×w1×C1/g，一共有C2/g个，输出数据为H2×W2×C2/g。</p>
<h3 id="heading"></h3>
<h3 id="dilation-convolution">Dilation Convolution</h3>
<p>dilation convolution （空洞卷积）最早应用于语义分割任务。语义分割是一个密集型预测任务，需要针对图像进行像素级别（pixel-wise）分类。</p>
<ul>
<li>pooling下采样操作导致的信息丢失是不可逆的，通常的分类识别模型，只需要预测每一类的概率，所以我们不需要考虑pooling会导致损失图像细节信息的问题，但是做像素级的预测时（譬如语义分割），就要考虑到这个问题了。</li>
<li>增加感受野，适应不同物体尺寸的变化。</li>
</ul>
<p>因此，空洞卷积能够在不降低空间分辨率的情况下，有效地提取不同尺度的上下文信息。</p>
<p>所以就要有一种卷积代替pooling的作用（成倍的增加感受野），而空洞卷积就是为了做这个的。通过卷积核插“0”的方式，它可以比普通的卷积获得更大的感受野，这个idea的motivation就介绍到这里。</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/31/v2-4959201e816888c6648f2e78cccfd253_hd.gif" alt=""></p>
<p>下面是常规的卷积操作</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/31/v2-d552433faa8363df84c53b905443a556_hd.gif" alt="img"></p>
<p>卷积核有个参数 diated rate （扩张率）</p>
<p>参考文献</p>
<p><a href="https://arxiv.org/pdf/1511.07122.pdf">MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS</a></p>
<h3 id="transposed-convolutions">Transposed Convolutions</h3>
<p><strong>反卷积和上采样的区别</strong></p>
<p>转置卷积 (transposed convolution) 一般又称为反卷积（Deconvolution）。可以理解为卷积的逆过程，实现上采用转置卷积核的方式。但是这种反卷积的叫法不合适，转置卷积不能实现卷积操作的逆过程，只能还原 shape 打下，不能还原 value 的大小。</p>
<p>普通的卷积和池化会缩小图像的尺寸，有时候为了得到和原图等大的分割图，需要进行上采样卷积。</p>
<p><img src="http://123.56.8.10:8899/images/2021/04/26/1EF52InXEbUhf12149452e9f1f39.gif" alt="img"></p>
<p>从卷积到转置卷积的理解可以查看这里：https://www.malaoshi.top/show_1EF52HLXBmqf.html</p>
<p>（对于反卷积，以下有不同的形态）</p>
<p><img src="http://123.56.8.10:8899/images/2021/04/26/njW8CF.gif" alt="transposed convolution"></p>
<p>反卷积的使用场景一般在语义分割和生成模型（比如说GAN ）中使用较多。</p>
<p>upPooling： 当upPooling 的时候填充的是0。</p>
<p>（这里针对MaxPooling）可以看到UnPooling操作相当于在进行MaxPooling操作时候对局部最大值的出现位置进行了记录，然后在反向过程补全的时候将最大值填充回原处，然后剩余的地方全部填充零。</p>
<blockquote>
<p>这个是需要记录位置的</p>
</blockquote>
<p><img src="http://123.56.8.10:8899/images/2021/04/26/image-20210426170024653.png" alt="UpPooling"></p>
<h3 id="deformable-convolutional-networks">Deformable convolutional networks</h3>
<p>deformable convolutional networks (DCN)</p>
<p>Paper: <a href="https://arxiv.org/abs/1703.06211">Deformable Convolutional Networks</a></p>
<p>github: <a href="https://github.com/CharlesShang/DCNv2">https://github.com/CharlesShang/DCNv2</a></p>
<p>这个论文也是非常的简单直白，关键是从结果上也是非常有效的。</p>
<blockquote>
<p><em>Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, de- formable convolution and deformable RoI pooling.</em></p>
<p>这个是 DCN (deformable convolutional networks) 的初衷：大多使用 fixed filter，作者提出可以使用可变的 filter。</p>
</blockquote>
<blockquote>
<p>The first is to build the training datasets with sufficient desired variations. This is usually realized by augmenting the existing data samples, <em>e.g</em>., by affine transformation.  (extensive data augmentation)</p>
<p>The second is to use transformation-invariant features and algorithms.</p>
<p>提高模型的效果的两种方法：（1）大数据去覆盖所有的 case（2）提高算法模型本身的泛化性能</p>
</blockquote>
<p><img src="http://123.56.8.10:8899/images/2021/11/17/image-20211117193417167.png" alt="image-20211117193417167" style="zoom:90%;" /></p>
<blockquote>
<p>左图是 standard 3*3 conv 和 DCN conv 直观的对比；右图是具体实现时候的细节：</p>
</blockquote>
<blockquote>
<p>As illustrated in Figure 2, the offsets are obtained by applying a convolutional layer over the same input feature map. The convolution kernel is of the same spatial resolu- tion and dilation as those of the current convolutional layer (<em>e.g</em>., also 3 × 3 with dilation 1 in Figure 2). The output offset fields have the same spatial resolution with the input feature map. The channel dimension 2N corresponds to N 2D offsets. During training, both the convolutional kernels for generating the output features and the offsets are learned simultaneously. To learn the offsets, the gradients are back- propagated through the bilinear operations in Eq. (3) and Eq. (4). It is detailed in appendix A.</p>
<p>实现的细节还不是很懂</p>
<p>特征图，再额外经过一个卷积层，生成 offset 的结果，然后把这个 offset 和特征图融合。</p>
</blockquote>
<p>可解释性</p>
<p><img src="http://123.56.8.10:8899/images/2021/11/17/image-20211117194844652.png" alt="image-20211117194844652"></p>
<blockquote>
<p>从效果图上看，这个还是有提升的。并且不会增加 train 和 test 的成本。</p>
<p>可以看出在几乎不增加参数量与运行时间的情况下，论文的方法带来了较大的性能提升。</p>
</blockquote>
<p><img src="http://123.56.8.10:8899/images/2021/11/17/image-20211117194920364.png" alt="image-20211117194920364"></p>
<blockquote>
<p>deformable Convolution可变卷积针对的对象是卷积本身，因此膨胀卷积，3D卷积都可以用可变卷积的形式</p>
</blockquote>
<p>参考文献（TODO）</p>
<p><a href="">卷积神经网络中十大拍案叫绝的操作！</a></p>
<h3 id="sparse-convolution">sparse convolution</h3>
<p><a href="">SECOND- Sparsely Embedded Convolutional Detection</a></p>
<blockquote>
<p>point cloud data contain accurate depth information and can be generated by LiDAR or RGB-D camers.</p>
<p>VoxelNet 效果比较好，但很慢，所以需要使用 SPConv 而不是传统的 Conv</p>
</blockquote>
<blockquote>
<p>our sparse-convolution-based detector achieves a factor-of-4 speed enhancement during training on the KITTI dataset and a factor-of-3 improvement in the speed of inference.</p>
</blockquote>
<p>投影到 BEV （Bird&rsquo;s-Eye-View-Based） 或者前视（front-view）</p>
<p><a href="">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</a></p>
<p>这个提出了其中的一种 sparse convolutional network</p>
<p><img src="http://123.56.8.10:8899/images/2021/10/28/image-20211028134727033.png" alt="image-20211028134727033" style="zoom:50%;" /></p>
<blockquote>
<p>First, we pad the input with (f − 1)/2 zeros on each side, so that the output will have the same size as the input. Next, we restrict an output site to be active if and only if the site at the corresponding site in the input is active (<em>i.e.</em>, if the central site in the receptive field is ac- tive). Whenever an output site is determined to be active, its output feature vector is computed by the SSC convo- lution</p>
</blockquote>
<p>参考资料</p>
<p><a href="https://pdfs.semanticscholar.org/5125/a16039cabc6320c908a4764f32596e018ad3.pdf">SECOND: Sparsely Embedded Convolutional Detection</a></p>
<blockquote>
<p>这个已经读过了，但不懂稀疏conv 的操作和细节</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/1711.10275.pdf">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</a></p>
<p>英文版：https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1</p>
<p>中文版：<a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1635304842&amp;ver=3399&amp;signature=sh4gXCJAzgzWMrBG09QfCHXCxn2zkqDdnIufwbdJS9smJPOnxNzYvOzjGbMIrV*bg0NCFBCbQ-iDGdNE-q0KqPMRC0dMV1BfZ6m99ddQaXAjRLO*kuDfLNSNHad2tpgn&amp;new=1">通俗易懂的解释Sparse Convolution过程</a></p>
<blockquote>
<p>虽然中文版也没有看懂</p>
</blockquote>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">jijeng</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2020-04-07
        
    </span>
  </p>
  
  
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="http://47.94.35.231:9998/blog_imgs/wechatpay.png">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="http://47.94.35.231:9998/blog_imgs/alipay.png">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/cnn/">cnn</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/cluster_algorithm/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Cluster algorithm</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/gpu/">
            <span class="next-text nav-default">深度学习中的 GPU</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="http://jijeng.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2018 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span>jijeng</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
