<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Ensemble - Jijeng&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="jijeng" /><meta name="description" content="å°†é›†æˆå­¦ä¹ åˆ†ä¸ºåŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•ã€é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•å’ŒåŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•ã€‚
 Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this article.
" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.79.1 with theme even" />


<link rel="canonical" href="http://jijeng.github.io/post/ensemble/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Ensemble" />
<meta property="og:description" content="å°†é›†æˆå­¦ä¹ åˆ†ä¸ºåŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•ã€é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•å’ŒåŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•ã€‚

Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this article.
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://jijeng.github.io/post/ensemble/" />
<meta property="article:published_time" content="2018-02-05T21:27:04+08:00" />
<meta property="article:modified_time" content="2018-02-05T21:27:04+08:00" />
<meta itemprop="name" content="Ensemble">
<meta itemprop="description" content="å°†é›†æˆå­¦ä¹ åˆ†ä¸ºåŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•ã€é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•å’ŒåŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•ã€‚

Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this article.
">
<meta itemprop="datePublished" content="2018-02-05T21:27:04+08:00" />
<meta itemprop="dateModified" content="2018-02-05T21:27:04+08:00" />
<meta itemprop="wordCount" content="3381">



<meta itemprop="keywords" content="ensemble," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Ensemble"/>
<meta name="twitter:description" content="å°†é›†æˆå­¦ä¹ åˆ†ä¸ºåŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•ã€é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•å’ŒåŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•ã€‚

Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this article.
"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Jijeng&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Jijeng&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Ensemble</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-02-05 </span>
        <div class="post-category">
            <a href="/categories/machine-learning/"> machine learning </a>
            </div>
          <span class="more-meta"> çº¦ 3381 å­— </span>
          <span class="more-meta"> é¢„è®¡é˜…è¯» 7 åˆ†é’Ÿ </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">æ–‡ç« ç›®å½•</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#åŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•">åŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•</a></li>
        <li><a href="#é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•">é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•</a></li>
        <li><a href="#åŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•">åŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•</a></li>
        <li><a href="#ä»£ç å®ç°">ä»£ç å®ç°</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>å°†é›†æˆå­¦ä¹ åˆ†ä¸ºåŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•ã€é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•å’ŒåŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•ã€‚</p>
<blockquote>
<p>Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this article.</p>
</blockquote>
<h2 id="åŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•">åŸºæœ¬é›†æˆå­¦ä¹ æ–¹æ³•</h2>
<p>ï¼ˆ1ï¼‰Max Voting</p>
<p>å¤šäººæŠ•ç¥¨æœºåˆ¶ï¼Œä½¿ç”¨æ‰€æœ‰çš„æ¨¡å‹é¢„æµ‹ç»“æœçš„å¤šæ•°ï¼Œå¸¸ç”¨äºåˆ†ç±»é—®é¢˜ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model1</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">model3</span><span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">pred1</span><span class="o">=</span><span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">pred2</span><span class="o">=</span><span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">pred3</span><span class="o">=</span><span class="n">model3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="n">final_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">)):</span>
    <span class="n">final_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">final_pred</span><span class="p">,</span> <span class="n">mode</span><span class="p">([</span><span class="n">pred1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">pred2</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">pred3</span><span class="p">[</span><span class="n">i</span><span class="p">]]))</span>
</code></pre></td></tr></table>
</div>
</div><p>ï¼ˆ2ï¼‰Averaging</p>
<blockquote>
<p>Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.
å¦‚æœæ˜¯å›å½’é—®é¢˜ï¼Œé‚£ä¹ˆæ±‚è§£å‡å€¼ï¼›å¦‚æœæ˜¯åˆ†ç±»é—®é¢˜ï¼Œé‚£ä¹ˆæ±‚è§£æ¦‚ç‡</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model1</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">model3</span><span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">pred1</span><span class="o">=</span><span class="n">model1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">pred2</span><span class="o">=</span><span class="n">model2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">pred3</span><span class="o">=</span><span class="n">model3</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="n">finalpred</span><span class="o">=</span><span class="p">(</span><span class="n">pred1</span><span class="o">+</span><span class="n">pred2</span><span class="o">+</span><span class="n">pred3</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>
</code></pre></td></tr></table>
</div>
</div><p>ï¼ˆ3ï¼‰Weighted Averaging</p>
<p>å’Œä¸Šé¢çš„æ€æƒ³ç±»ä¼¼ï¼Œæ˜¯æœ‰æƒé‡çš„ï¼Œæƒé‡çš„ä¾æ®æ˜¯æ¨¡å‹çš„å‡†ç¡®ç‡ç­‰æŒ‡æ ‡ã€‚å¦‚æœæ¨¡å‹æ•ˆæœè¶Šå¥½ï¼Œé‚£ä¹ˆæƒé‡è¶Šé«˜ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model1</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">model3</span><span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">model3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">pred1</span><span class="o">=</span><span class="n">model1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">pred2</span><span class="o">=</span><span class="n">model2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">pred3</span><span class="o">=</span><span class="n">model3</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="n">finalpred</span><span class="o">=</span><span class="p">(</span><span class="n">pred1</span><span class="o">*</span><span class="mf">0.3</span><span class="o">+</span><span class="n">pred2</span><span class="o">*</span><span class="mf">0.3</span><span class="o">+</span><span class="n">pred3</span><span class="o">*</span><span class="mf">0.4</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•">é«˜çº§é›†æˆå­¦ä¹ æ–¹æ³•</h2>
<p>è¿™å°±å¼•å‡ºäº†å¦‚ä½•ç»„åˆè¿™äº›æ¨¡å‹çš„é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¸‰ç§ä¸»è¦çš„æ—¨åœ¨ç»„åˆå¼±å­¦ä¹ å™¨çš„ã€Œå…ƒç®—æ³•ã€ï¼š</p>
<ul>
<li>baggingï¼Œè¯¥æ–¹æ³•é€šå¸¸è€ƒè™‘çš„æ˜¯åŒè´¨å¼±å­¦ä¹ å™¨ï¼Œç›¸äº’ç‹¬ç«‹åœ°å¹¶è¡Œå­¦ä¹ è¿™äº›å¼±å­¦ä¹ å™¨ï¼Œå¹¶æŒ‰ç…§æŸç§ç¡®å®šæ€§çš„å¹³å‡è¿‡ç¨‹å°†å®ƒä»¬ç»„åˆèµ·æ¥ã€‚</li>
<li>boostingï¼Œè¯¥æ–¹æ³•é€šå¸¸è€ƒè™‘çš„ä¹Ÿæ˜¯åŒè´¨å¼±å­¦ä¹ å™¨ã€‚å®ƒä»¥ä¸€ç§é«˜åº¦è‡ªé€‚åº”çš„æ–¹æ³•é¡ºåºåœ°å­¦ä¹ è¿™äº›å¼±å­¦ä¹ å™¨ï¼ˆæ¯ä¸ªåŸºç¡€æ¨¡å‹éƒ½ä¾èµ–äºå‰é¢çš„æ¨¡å‹ï¼‰ï¼Œå¹¶æŒ‰ç…§æŸç§ç¡®å®šæ€§çš„ç­–ç•¥å°†å®ƒä»¬ç»„åˆèµ·æ¥ã€‚</li>
<li>stackingï¼Œè¯¥æ–¹æ³•é€šå¸¸è€ƒè™‘çš„æ˜¯å¼‚è´¨å¼±å­¦ä¹ å™¨ï¼Œå¹¶è¡Œåœ°å­¦ä¹ å®ƒä»¬ï¼Œå¹¶é€šè¿‡è®­ç»ƒä¸€ä¸ªã€Œå…ƒæ¨¡å‹ã€å°†å®ƒä»¬ç»„åˆèµ·æ¥ï¼Œæ ¹æ®ä¸åŒå¼±æ¨¡å‹çš„é¢„æµ‹ç»“æœè¾“å‡ºä¸€ä¸ªæœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚</li>
</ul>
<p>ï¼ˆ1ï¼‰Bagging</p>
<p>æ€æƒ³ï¼šä½¿ç”¨åŒè´¨æ¨¡å‹å’Œç›¸åŒçš„æ•°æ®é›†ï¼Œå¤§æ¦‚ç‡å¾—åˆ°çš„æ˜¯ç›¸åŒçš„ç»“æœï¼Œè¿™ä¸ªæ˜¯åboosting ç®—æ³•ä¸­ä½¿ç”¨äº†ä¸€ç§é‡‡æ ·æ–¹å¼ <code>Bootstrapping</code>ã€‚</p>
<blockquote>
<p>Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, <strong>with replacement</strong>.</p>
</blockquote>
<p>ç®—æ³•æ­¥éª¤ï¼š</p>
<ol>
<li>ä»è®­ç»ƒé›† $ğ‘†$ä¸­æœ‰æ”¾å›çš„éšæœºé€‰å–æ•°æ®é›† $ğ‘€(âˆ£ğ‘€âˆ£&lt;âˆ£ğ‘†âˆ£) $;</li>
<li>ç”Ÿæˆä¸€ä¸ªåˆ†ç±»æ¨¡å‹ $ğ¶ $;</li>
<li>é‡å¤ä»¥ä¸Šæ­¥éª¤ $m$æ¬¡ï¼Œå¾—åˆ°$m$ä¸ªåˆ†ç±»æ¨¡å‹ $ğ¶_1$, $ğ¶_2 $,&hellip;, $ğ¶_m $;</li>
<li>å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œæ¯ä¸€ä¸ªæ¨¡å‹æŠ•ç¥¨å†³å®šï¼Œå°‘æ•°æœä»å¤šæ•°åŸåˆ™; å¯¹äºå›å½’é—®é¢˜ï¼Œå–å¹³å‡å€¼ã€‚</li>
</ol>
<p>ï¼ˆ2ï¼‰Boosting</p>
<p>æ€æƒ³ï¼šå¦‚æœæ¨¡å‹æœ¬èº«çš„å‡†ç¡®ç‡å°±ä¸é«˜ï¼Œé‚£ä¹ˆå¤šä¸ªæ¨¡å‹ç»„åˆèµ·æ¥ä¸è§å¾—å¥½ã€‚æ‰€ä»¥Boosting çš„æ€æƒ³æ˜¯åœ¨é’ˆå¯¹ä¸Šä¸€ä¸ªæ¨¡å‹çš„é”™è¯¯æ¥è®­ç»ƒå½“ä¸‹çš„æ¨¡å‹ã€‚</p>
<p>ï¼ˆ3ï¼‰Stacking</p>
<p>ä»¥äºŒå±‚çš„stacking ä½œä¸ºè®²è§£ã€‚</p>
<p>ç®—æ³•æ­¥éª¤ï¼š</p>
<p>1). The train set is split into 10 parts.
2). A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.
3). The base model (in this case, decision tree) is then fitted on the whole train dataset.
4). Using this model, predictions are made on the test set.
5). Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set.
6). The predictions from the train set are used as <strong>features to build a new model.</strong>
7). This model is used to make final predictions on the test prediction set.</p>
<p>å…³é”®æ­¥éª¤æ˜¯ä»¥ç¬¬ä¸€å±‚æ¨¡å‹çš„è¾“å‡ºç»“æœä½œä¸ºç¬¬äºŒæ¬¡çš„è¾“å…¥ã€‚ç»™å‡ºä»£ç å®ä¾‹ã€‚</p>
<p>å®šä¹‰ä¸€ä¸ªé€šç”¨å‡½æ•°ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">Stacking</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">train</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test</span><span class="p">,</span><span class="n">n_fold</span><span class="p">):</span>
   <span class="n">folds</span><span class="o">=</span><span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_fold</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
   <span class="n">test_pred</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">),</span><span class="nb">float</span><span class="p">)</span>
   <span class="n">train_pred</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="nb">float</span><span class="p">)</span>
   <span class="k">for</span> <span class="n">train_indices</span><span class="p">,</span><span class="n">val_indices</span> <span class="ow">in</span> <span class="n">folds</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">):</span>
      <span class="n">x_train</span><span class="p">,</span><span class="n">x_val</span><span class="o">=</span><span class="n">train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_indices</span><span class="p">],</span><span class="n">train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>
      <span class="n">y_train</span><span class="p">,</span><span class="n">y_val</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_indices</span><span class="p">],</span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">val_indices</span><span class="p">]</span>

      <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
      <span class="n">train_pred</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_pred</span><span class="p">,</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_val</span><span class="p">))</span>
      <span class="n">test_pred</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_pred</span><span class="p">,</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">test_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">train_pred</span>
</code></pre></td></tr></table>
</div>
</div><p>ç¬¬ä¸€å±‚æœ‰ä¸¤ä¸ªåŸºæœ¬çš„æ¨¡å‹ï¼šå†³ç­–æ ‘å’ŒKæœ€è¿‘é‚»ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model1</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_pred1</span> <span class="p">,</span><span class="n">train_pred1</span><span class="o">=</span><span class="n">Stacking</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model1</span><span class="p">,</span><span class="n">n_fold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span><span class="n">test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">train_pred1</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">train_pred1</span><span class="p">)</span>
<span class="n">test_pred1</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_pred1</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">test_pred2</span> <span class="p">,</span><span class="n">train_pred2</span><span class="o">=</span><span class="n">Stacking</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model2</span><span class="p">,</span><span class="n">n_fold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span><span class="n">test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">train_pred2</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">train_pred2</span><span class="p">)</span>
<span class="n">test_pred2</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_pred2</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>ç¬¬ä¸€å±‚çš„ä¸¤ä¸ªæ¨¡å‹å¾—åˆ°äº†predictçš„å€¼ï¼Œç„¶åå°†ç»“æœæ‹¼æ¥èµ·æ¥ä½œä¸ºç¬¬äºŒå±‚æ¨¡å‹çš„è¾“å…¥æ•°æ®é›†ï¼Œç¬¬äºŒå±‚æ¨¡å‹çš„label è¿˜æ˜¯åŸå§‹è®­ç»ƒæ•°æ®é›†çš„labelã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">train_pred1</span><span class="p">,</span> <span class="n">train_pred2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># åŸºæœ¬æ¨¡å‹prediction çš„ç»“æœå½“åšè®­ç»ƒçš„è¾“å…¥</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">test_pred1</span><span class="p">,</span> <span class="n">test_pred2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># ä¸ºäº†ä¿è¯åŒåˆ†å¸ƒï¼Œè¿™é‡Œå¯¹test æ•°æ®é›†ä¹Ÿåšç›¸åŒçš„è½¬æ¢</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span> <span class="c1"># æ•°æ®çš„label è¿˜æ˜¯ä½œä¸ºç¬¬ä¸€å±‚æ¨¡å‹çš„label </span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">df_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="c1"># æœ€åæ˜¯æ¨¡å‹çš„è¾“å‡º</span>
</code></pre></td></tr></table>
</div>
</div><p>åœ¨ test æ•°æ®é›†ä¸Šçš„é¢„æµ‹ï¼Œéœ€è¦ç»è¿‡ä¸¤å±‚æ¨¡å‹ï¼šé¦–å…ˆè¾“å…¥åˆ°ï¼ˆå†³ç­–æ ‘å’Œkæœ€è¿‘é‚»ï¼‰çš„æ¨¡å‹ä¸­ï¼Œå…¶ç»“æœå†è¾“å…¥åˆ°ç¬¬äºŒå±‚æ¨¡å‹å†³ç­–æ ‘ä¸­ï¼Œå†³ç­–æ ‘çš„é¢„æµ‹å€¼å°±æ˜¯æœ€åçš„ç»“æœã€‚</p>
<p>æ¯ä¸€è½®æ ¹æ®ä¸Šä¸€è½®çš„åˆ†ç±»ç»“æœåŠ¨æ€è°ƒæ•´æ¯ä¸ªæ ·æœ¬åœ¨åˆ†ç±»å™¨ä¸­çš„æƒé‡ï¼Œè®­ç»ƒå¾—åˆ°kä¸ªå¼±åˆ†ç±»å™¨ï¼Œä»–ä»¬éƒ½æœ‰å„è‡ªçš„æƒé‡ï¼Œé€šè¿‡åŠ æƒç»„åˆçš„æ–¹å¼å¾—åˆ°æœ€ç»ˆçš„åˆ†ç±»ç»“æœ(ç»¼åˆæ‰€æœ‰çš„åŸºæ¨¡å‹é¢„æµ‹ç»“æœ)ã€‚</p>
<p>ï¼ˆ4ï¼‰Blending</p>
<p>Blending ç›¸æ¯”äºStackingè€Œè¨€ï¼ŒBlending æ˜¯åœ¨è®­ç»ƒé›†ä¸Štrainï¼Œåœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Špredictionï¼Œç„¶åä½¿ç”¨éªŒè¯é›†å’Œæµ‹è¯•é›†çš„predictionä½œä¸º features å»å­¦ä¹ ä¸‹ä¸€ä¸ªæ¨¡å‹ã€‚</p>
<p>ç®—æ³•æ­¥éª¤ï¼š</p>
<p>1). The train set is split into training and validation sets.
2). Model(s) are fitted on the training set.
3). The predictions are made on the validation set and the test set.
4). The validation set and its predictions are used as features to build a new model.
5). This model is used to make final predictions on the test and meta-features.</p>
<p>åŒæ ·ç»™å‡ºäº†sample codes</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model1</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">model1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">val_pred1</span><span class="o">=</span><span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span>
<span class="n">test_pred1</span><span class="o">=</span><span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">val_pred1</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">val_pred1</span><span class="p">)</span>
<span class="n">test_pred1</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_pred1</span><span class="p">)</span>

<span class="n">model2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">val_pred2</span><span class="o">=</span><span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span>
<span class="n">test_pred2</span><span class="o">=</span><span class="n">model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">val_pred2</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">val_pred2</span><span class="p">)</span>
<span class="n">test_pred2</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_pred2</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>ç¬¬äºŒå±‚ä½¿ç”¨äº†é€»è¾‘å›å½’åœ¨test set ä¸Šè¿›è¡Œé¢„æµ‹ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">df_val</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_val</span><span class="p">,</span> <span class="n">val_pred1</span><span class="p">,</span><span class="n">val_pred2</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_test</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_test</span><span class="p">,</span> <span class="n">test_pred1</span><span class="p">,</span><span class="n">test_pred2</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_val</span><span class="p">,</span><span class="n">y_val</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">df_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="åŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•">åŸºäºé›†æˆå­¦ä¹ çš„ç®—æ³•</h2>
<p>Boosting å’ŒBagging æ˜¯æœ€å¸¸ä½¿ç”¨çš„ä¸¤ç±»é›†æˆç®—æ³•ã€‚Bagging algorithmsçš„ä»£è¡¨æ˜¯Random foresã€‚Boosting algorithmsæœ‰ä»¥ä¸‹å‡ ç§ï¼š</p>
<ul>
<li>AdaBoost</li>
<li>GBM</li>
<li>XGBM</li>
<li>Light GBM</li>
<li>CatBoost</li>
</ul>
<p>ï¼ˆ1ï¼‰Random Forest</p>
<p>Random Forest çš„ç®—æ³•æ­¥éª¤ï¼š
1). Random subsets are created from the original dataset (bootstrapping).
2). At each node in the decision tree, only a random set of features are considered to decide the best split.
3). A decision tree model is fitted on each of the subsets.
4). The final prediction is calculated by averaging the predictions from all decision trees.</p>
<p>æ€»ç»“æ¥è¯´ï¼Œéšæœºæ£®æ—<strong>éšæœº</strong> é€‰æ‹©æ•°æ®ç‚¹å’Œç‰¹å¾ï¼Œç„¶åç»„æˆäº†å¤šæ£µæ ‘çš„é›†åˆï¼ˆæ£®æ—ï¼‰</p>
<p>å¸¸è§çš„è¶…å‚æ•°
1 ). n_estimators (å­æ ‘çš„ä¸ªæ•°)</p>
<ul>
<li>It defines the number of decision trees to be created in a random forest.</li>
<li>Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.</li>
</ul>
<p>2 ). max_features (ä½¿ç”¨æœ€å¤šç‰¹å¾çš„æ•°é‡è¿›è¡Œå»ºç«‹æ ‘)</p>
<ul>
<li>It defines the maximum number of features allowed for the split in each decision tree.</li>
<li>Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.</li>
</ul>
<p>3 ). max_depth ( æ˜¯ä¸€ä¸ªæ ‘çš„æœ€å¤§æ·±åº¦ï¼Œä¹Ÿæ˜¯æ‰€æœ‰æ ‘çš„æœ€å¤§æ·±åº¦)</p>
<ul>
<li>The maximum depth of the tree.</li>
</ul>
<p>4 ). min_samples_leaf</p>
<ul>
<li>This defines the minimum number of samples required to be at a leaf node.</li>
<li>Smaller leaf size makes the model more prone to capturing noise in train data.</li>
</ul>
<p>ï¼ˆ2ï¼‰AdaBoost</p>
<blockquote>
<p>Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.
æå‡ç­–ç•¥ä¸»è¦æ˜¯ä»åˆ†é”™ç±»çš„æ ·æœ¬è§’åº¦æå‡ï¼Œé€šè¿‡ç»™äºˆæ›´å¤§çš„æƒé‡ã€‚ä¸‹ä¸€ä¸ªæ¨¡å‹é¢„æµ‹çš„æ—¶å€™ï¼Œç€é‡è¯¥æ ·æœ¬ã€‚</p>
</blockquote>
<p>ï¼ˆ3ï¼‰Gradient Boosting (GBM)</p>
<blockquote>
<p>Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.
æå‡ç­–ç•¥æ˜¯ä»æ¢¯åº¦è§’åº¦è€ƒè™‘ï¼Œä½¿ç”¨å†³ç­–æ ‘æ¨¡å‹å‡å°‘lossï¼Œè¿›è€Œæé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p>ä¼˜ç‚¹</p>
<ol>
<li>åœ¨åˆ†å¸ƒç¨ å¯†çš„æ•°æ®é›†ä¸Šï¼Œæ³›åŒ–èƒ½åŠ›å’Œè¡¨è¾¾èƒ½åŠ›éƒ½å¾ˆå¥½ï¼Œè¿™ä½¿å¾—GBDTåœ¨Kaggleçš„ä¼—å¤šç«èµ›ä¸­ï¼Œç»å¸¸ååˆ—æ¦œé¦–ã€‚</li>
</ol>
<p>ç¼ºç‚¹</p>
<ol>
<li>GBDTåœ¨é«˜ç»´ç¨€ç–çš„æ•°æ®é›†ä¸Šï¼Œè¡¨ç°ä¸å¦‚æ”¯æŒå‘é‡æœºæˆ–è€…ç¥ç»ç½‘ç»œã€‚ï¼ˆæ‰€ä»¥è¯´è¿™ç§æ ‘çš„æ¨¡å‹æ˜¯æœ‰åˆ©äºå¤„ç†è¿ç»­æ•°å€¼ï¼Œ å¦‚æœæ˜¯ one-hot å°±ä¸å»ºè®®ä½¿ç”¨gbdtï¼‰</li>
<li>GBDTåœ¨å¤„ç†æ–‡æœ¬åˆ†ç±»ç‰¹å¾é—®é¢˜ä¸Šï¼Œç›¸å¯¹å…¶ä»–æ¨¡å‹çš„ä¼˜åŠ¿ä¸å¦‚å®ƒåœ¨å¤„ç†æ•°å€¼ç‰¹å¾æ—¶æ˜æ˜¾ã€‚</li>
</ol>
<p>ï¼ˆ4ï¼‰XGBoost</p>
<blockquote>
<p>XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as â€˜regularized boostingâ€˜ technique.
æå‡é€Ÿåº¦å’Œä½¿ç”¨æ­£åˆ™æ–¹å¼å¤„ç†è¿‡æ‹Ÿåˆã€‚</p>
</blockquote>
<p>ä¸»è¦é‡‡ç”¨ä»¥ä¸‹çš„æŠ€æœ¯ï¼š
1). Regularization:</p>
<ul>
<li>Standard GBM implementation has no regularisation like XGBoost.</li>
<li>Thus XGBoost also helps to reduce overfitting.
2). Parallel Processing:</li>
<li>XGBoost implements parallel processing and is faster than GBM .</li>
<li>XGBoost also supports implementation on Hadoop.
3). High Flexibility:</li>
<li>XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.
4). Handling Missing Values:</li>
<li>XGBoost has an in-built routine to handle missing values.
5). Tree Pruning:</li>
<li>XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.
6). Built-in Cross-Validation:</li>
<li>XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.</li>
</ul>
<p><strong>GBDTä¸XGboostè”ç³»ä¸åŒºåˆ«</strong></p>
<p>(1) GBDTæ˜¯æœºå™¨å­¦ä¹ ç®—æ³•ï¼ŒXGBoostæ˜¯è¯¥ç®—æ³•çš„å·¥ç¨‹å®ç°ã€‚
(2) åœ¨ä½¿ç”¨CARTä½œä¸ºåŸºåˆ†ç±»å™¨æ—¶ï¼ŒXGBoostæ˜¾å¼åœ°åŠ å…¥äº†æ­£åˆ™é¡¹æ¥æ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæœ‰åˆ©äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
(3) GBDTåœ¨æ¨¡å‹è®­ç»ƒæ—¶åªä½¿ç”¨äº†ä»£ä»·å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°ä¿¡æ¯ï¼ŒXGBoostå¯¹ä»£ä»·å‡½æ•°è¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå¯ä»¥åŒæ—¶ä½¿ç”¨ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°ã€‚
(4) ä¼ ç»Ÿçš„GBDTé‡‡ç”¨CARTä½œä¸ºåŸºåˆ†ç±»å™¨ï¼ŒXGBoostæ”¯æŒå¤šç§ç±»å‹çš„åŸºåˆ†ç±»å™¨ï¼Œæ¯”å¦‚çº¿æ€§åˆ†ç±»å™¨ã€‚
(5) ä¼ ç»Ÿçš„GBDTåœ¨æ¯è½®è¿­ä»£æ—¶ä½¿ç”¨å…¨éƒ¨çš„æ•°æ®ï¼ŒXGBooståˆ™é‡‡ç”¨äº†ä¸éšæœºæ£®æ—ç›¸ä¼¼çš„ç­–ç•¥ï¼Œæ”¯æŒå¯¹æ•°æ®è¿›è¡Œé‡‡æ ·ã€‚
(6) ä¼ ç»Ÿçš„GBDTæ²¡æœ‰è®¾è®¡å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¤„ç†ï¼ŒXGBoostèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å‡ºç¼ºå¤±å€¼çš„å¤„ç†ç­–ç•¥ã€‚</p>
<p>ï¼ˆ6ï¼‰Light GBM</p>
<blockquote>
<p>** Light GBM beats all the other algorithms when the dataset is extremely large**. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.
LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern. The images below will help you understand the difference in a better way.</p>
</blockquote>
<p>æ•ˆæœä¸Šä½¿ç”¨åŸºäºå¶å­çš„æ ‘çš„ç”Ÿé•¿æ–¹å¼ï¼Œè€Œéå±‚æ¬¡çš„ç”Ÿæˆæ–¹å¼ã€‚</p>
<p>ï¼ˆ7ï¼‰CatBoost</p>
<blockquote>
<p>Handling categorical variables is a tedious process, especially when you have a large number of such variables. When your categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difficult to work with the dataset.
CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms. Here is an article that explains CatBoost in detail.</p>
</blockquote>
<p>ä»åå­—ä¸Šå°±çŸ¥é“åœ¨å¤„ç†ç±»åˆ«ä¿¡æ¯å¾ˆå¤šçš„æ•°æ®é›†ä¸­ï¼Œä¸éœ€è¦é¢„å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨ã€‚</p>
<h2 id="ä»£ç å®ç°">ä»£ç å®ç°</h2>
<p>1). <a href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/">A Comprehensive Guide to Ensemble Learning (with Python codes)</a>ï¼šå…³äºé›†æˆå­¦ä¹ å…¨é¢çš„è®²è§£ã€‚
2). <a href="https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/">How to Develop a Stacking Ensemble for Deep Learning Neural Networks in Python With Keras</a>ï¼šæ­å»ºçš„æ˜¯CNN çš„ç½‘ç»œã€‚
3). <a href="https://machinelearningmastery.com/implementing-stacking-scratch-python/">How to Implement Stacked Generalization (Stacking) From Scratch With Python</a>ï¼š ä»…ä»…ä½¿ç”¨äº†pythonï¼Œæ²¡æœ‰ä½¿ç”¨å…¶ä»–çš„å„ç§æœºå™¨å­¦ä¹ çš„åº“ã€‚</p>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">æ–‡ç« ä½œè€…</span>
    <span class="item-content">jijeng</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">ä¸Šæ¬¡æ›´æ–°</span>
    <span class="item-content">
        2018-02-05
        
    </span>
  </p>
  
  
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">èµèµæ”¯æŒ</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://ftp.bmp.ovh/imgs/2020/12/a67dbe80ab6832ca.png">
        <span>å¾®ä¿¡æ‰“èµ</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="https://ftp.bmp.ovh/imgs/2020/12/b575cd4858bd404d.png">
        <span>æ”¯ä»˜å®æ‰“èµ</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/ensemble/">ensemble</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/algorithm_practice_4/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Algorithm Practice (4)</span>
            <span class="prev-text nav-mobile">ä¸Šä¸€ç¯‡</span>
          </a>
        <a class="next" href="/post/binary_search/">
            <span class="next-text nav-default">Binary-Search</span>
            <span class="next-text nav-mobile">ä¸‹ä¸€ç¯‡</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="http://jijeng.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    ç”± <a class="hexo-link" href="https://gohugo.io">Hugo</a> å¼ºåŠ›é©±åŠ¨
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    ä¸»é¢˜ - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2018 - 
    2020<span class="heart"><i class="iconfont icon-heart"></i></span><span>jijeng</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
