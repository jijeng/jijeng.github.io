<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>ResNet and Inception V3 Understanding - Jijeng&#39;s Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="jijeng" /><meta name="description" content="本文介绍 ResNet的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。另外，和 Resnet 思路相反的是 Inception 系列。
" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.79.1 with theme even" />


<link rel="canonical" href="http://jijeng.github.io/post/resnet_understanding/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.c7bc1becf36bcf6a9ebd25d2947e43a2eb745ddb0c9a32b43126fd7fa460c351.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="ResNet and Inception V3 Understanding" />
<meta property="og:description" content="本文介绍 ResNet的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。另外，和 Resnet 思路相反的是 Inception 系列。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://jijeng.github.io/post/resnet_understanding/" />
<meta property="article:published_time" content="2019-06-21T16:39:06+08:00" />
<meta property="article:modified_time" content="2019-06-21T16:39:06+08:00" />
<meta itemprop="name" content="ResNet and Inception V3 Understanding">
<meta itemprop="description" content="本文介绍 ResNet的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。另外，和 Resnet 思路相反的是 Inception 系列。">
<meta itemprop="datePublished" content="2019-06-21T16:39:06+08:00" />
<meta itemprop="dateModified" content="2019-06-21T16:39:06+08:00" />
<meta itemprop="wordCount" content="4099">



<meta itemprop="keywords" content="resnet," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ResNet and Inception V3 Understanding"/>
<meta name="twitter:description" content="本文介绍 ResNet的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。另外，和 Resnet 思路相反的是 Inception 系列。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Jijeng&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Jijeng&#39;s Blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">ResNet and Inception V3 Understanding</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-06-21 </span>
        <div class="post-category">
            <a href="/categories/deep-learning/"> deep learning </a>
            </div>
          <span class="more-meta"> 约 4099 字 </span>
          <span class="more-meta"> 预计阅读 9 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#沐神讲解-resnet">沐神讲解 resnet</a></li>
            <li><a href="#resnet-网络">resnet 网络</a></li>
            <li><a href="#inception-网络">Inception 网络</a></li>
            <li><a href="#vgg网络">VGG网络</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>本文介绍 ResNet的背景，想要解决的问题，基本思路和框架 和对于其的一种解读方式。另外，和 Resnet 思路相反的是 Inception 系列。</p>
<h3 id="沐神讲解-resnet">沐神讲解 resnet</h3>
<p>防止梯度消失/梯度爆炸</p>
<ul>
<li>
<p>初始化：不要太小也不要太大</p>
</li>
<li>
<p>BN</p>
</li>
</ul>
<p>更大的网络并不是 overfitting，而是 train 和 test 时候网络都在变差。</p>
<p>如果浅层网络能做好，那么深层网络中新加的层从理论上说可以学成 identity mapping，至少不会变差。resnet 不会使得深层网络变得差。</p>
<p>residual connection</p>
<p>shortcut connection</p>
<p>经典的文章，不见得是原创，将之前的工作汇总一下，能够解决现在的某个新问题。</p>
<p>抄袭：最近的工作差不多，那叫做抄袭。如果 30年了，要客观一些，可以 to our best knowledge。</p>
<p>The image is resized with its shorter side ran- domly sampled in [256, 480] for scale augmentation</p>
<blockquote>
<p>按照短边（shorter side） 进行resize</p>
</blockquote>
<p>drop out 一般使用在全连接层中。（resnet 中没有使用 drop out，因为没有全连接）</p>
<p>flops：网路进行多少浮点运算。</p>
<p>lr 如果分成两段，那么开始的一段需要相对比较长一些，这样学习得更加充分一些（有积累？），然后学习率在降低。</p>
<p>res18，res34 是 plain 网络结构，没有 bottleneck</p>
<p><img src="http://123.56.8.10:8899/images/2022/01/20/image-20220120113125277.png" alt="image-20220120113125277"></p>
<blockquote>
<p>当你的通道数不变的时候，是左图的连接方式；当通道数发生变化的时候，比如右图开始 256 维度，通过 $1<em>1$ 降维成 64，然后相加的时候，通过 $1</em>1$ 升维，还原成 256 的维度。</p>
<p>左右的两个算法复杂度差不多。</p>
</blockquote>
<p>resnet50 使用 2048 维度作为向量的表示，从理论上可以做到更深。</p>
<p>Imagenet 图像大小在 300 以上</p>
<p>cifar 基本上是 32 *32 ，5w 样本</p>
<p>这个论文没有结论。（hhh）</p>
<p>（收敛 和 train 不动 是有区别的）使用 SGD 要保证梯度一直够大，这样才能一直优化 train 下去。</p>
<p>事后发现，resnet 训练比较快的原因：梯度保持得比较好。</p>
<p><img src="http://123.56.8.10:8899/images/2022/01/20/image-20220120120116840.png" alt="image-20220120120116840" style="zoom:50%;" /></p>
<p>为什么 transformer 中的参数量那么大，然而还没有过拟合？</p>
<p>过拟合还和网络的结构有关，比如 resnet 中加上了残差，使得网络结构复杂度降低，其实过拟合就没有那么严重。找到一个不那么复杂的模型去拟合数据。</p>
<p>挖坑，理论有创新新或者实践效果好，只要有一点好，那么就可以很好的论文。如果是真的好，那么大家会 follow，然后把坑给填完的。</p>
<h3 id="resnet-网络">resnet 网络</h3>
<p>（1）背景</p>
<p>网络的深度是容易出现梯度爆炸和梯度消失，造成网络的不收敛。一些方法已经在很大程度上可以缓解这个问题，比如使用 ReLU激活函数、 良好的权值初始化方法 、还有 intermediate normalization layers(即网络中间的batch normalization)。并且对于网络过程中的过拟合问题，也提出了一些办法如，使用 regularization、权值衰减和dropout方法。</p>
<p>但解决了深度网络收敛问题之后，又出现了另外一个问题。</p>
<p>（2）残差网络要解决的问题</p>
<p>一般来说在没有过拟合的情况下，可以逐步增加网络的深度。但在实验中发现了这样的问题。网络退化： 网络越深，训练误差越大。（accuracy开始饱和，原文中这样说的）这种退化并不是由于过拟合造成的，并且在适当深度模型中增加更多的层会导致更多的训练误差</p>
<p>（3）基本思路和结构</p>
<p>作者基于增加层如果为恒等映射那么更深层网络不应该比浅层网络产生更高错误率的思想</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/02/image-20210302165536275.png" alt="image-20210302165536275" style="zoom:50%;" /></p>
<p>如图所示左边的是传统的plain networks的结构，右边的是修改为ResNet的结构。
改变前目标： 训练 F(x) 逼近 H(x)
改变后目标：训练 F(x) 逼近 H(x) -x</p>
<p>即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换成F(x)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F(x)的优化 会比H(x)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果</p>
<p>在论文中尝试了 skip 2层或者 3层</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/02/5d43ea85bf14975363.jpg" alt="5.jpg"></p>
<p>ResNet 中 BasicBlock 和 Bottleneck 的区别（左图是 BasicBlock，右图是 Bottleneck）</p>
<p>BasicBlock 是 ResNet 中的一种网络结构，包含了残差支路（short-cut 支路），相比于传统卷积网络结构该支路能用于传递底层的信息，使得网络能能够训练得更深。</p>
<p>Bottleneck 是由 $1<em>1$, $3</em>3$ 和 $1<em>1$  三个卷积构成，其中 $1</em>1$ 卷积层作用是先减少再恢复维度，Bottleneck 用于特征降维，减少特征图的层数，减少参数量从而减少计算量。（一般在网络较深才使用 Bottleneck 结构，比如 Resnet18, Resnet34 用 BasicBlock，而 Resnet50, Resnet101 使用 Bottleneck）</p>
<p><strong>一种解读方式</strong></p>
<p>残差网络单元其中可以分解成右图的形式，从图中可以看出，残差网络其实是由多种路径组合的一个网络，直白了说，残差网络其实是很多并行子网络的组合，整个残差网络其实相当于一个多人投票系统（Ensembling）</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/02/5d43ea85b503b65896.png" alt="4.png">
从这可以看出其实ResNet是由大多数中度网络和一小部分浅度网络和深度网络组成的，</p>
<p>说明虽然表面上ResNet网络很深，但是其实起实际作用的网络层数并没有很深我们可以看出大多数的梯度其实都集中在中间的路径上，论文里称为effective path。 (网络越深，也是容易出现梯度消失或者梯度爆炸，这个是没有问题的)ResNet其实就是一个多人投票系统。</p>
<p>现在深度网络基本上分成两个方向，一个像 resnet 向着”深度“发展，一个向着”宽度“的inception network</p>
<p>（4）网路结构</p>
<p>以 pytorch 官方code 为例进行分析： <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py">https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py</a></p>
<p>resnet 家族中包括 resnet-18, resnet-34, resnet-50, resnet-101, resnet-152。可以发现都是在 <code>baseblock</code> 中通过增多 block 的数量实现的。</p>
<p><img src="http://123.56.8.10:8899/images/2021/03/02/image-20210302172226490.png" alt="image-20210302172226490"></p>
<p>如下是 basicBlock。conv $3*3$-BN-relu。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">BasicBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">expansion</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inplanes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">downsample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">base_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">norm_layer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span>
        <span class="k">if</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">base_width</span> <span class="o">!=</span> <span class="mi">64</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;BasicBlock only supports groups=1 and base_width=64&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dilation</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&#34;Dilation &gt; 1 not supported in BasicBlock&#34;</span><span class="p">)</span>
        <span class="c1"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">conv3x3</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">conv3x3</span><span class="p">(</span><span class="n">planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">identity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">identity</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="c1"># 这里体现了 resnet 的核心思想， skip-connecting，直接把 identity 加入了两层 conv 之后 </span>

        <span class="k">return</span> <span class="n">out</span>
</code></pre></td></tr></table>
</div>
</div><p>同一个家族中的不同 resnet 网络是通过传入了不同的参数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">progress</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResNet</span><span class="p">:</span>
    <span class="sa">r</span><span class="s2">&#34;&#34;&#34;ResNet-50 model from
</span><span class="s2">    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_.
</span><span class="s2">    Args:
</span><span class="s2">        pretrained (bool): If True, returns a model pre-trained on ImageNet
</span><span class="s2">        progress (bool): If True, displays a progress bar of the download to stderr
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="k">return</span> <span class="n">_resnet</span><span class="p">(</span><span class="s1">&#39;resnet50&#39;</span><span class="p">,</span> <span class="n">Bottleneck</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">pretrained</span><span class="p">,</span> <span class="n">progress</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resnet101</span><span class="p">(</span><span class="n">pretrained</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">progress</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResNet</span><span class="p">:</span>
    <span class="sa">r</span><span class="s2">&#34;&#34;&#34;ResNet-101 model from
</span><span class="s2">    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_.
</span><span class="s2">    Args:
</span><span class="s2">        pretrained (bool): If True, returns a model pre-trained on ImageNet
</span><span class="s2">        progress (bool): If True, displays a progress bar of the download to stderr
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="k">return</span> <span class="n">_resnet</span><span class="p">(</span><span class="s1">&#39;resnet101&#39;</span><span class="p">,</span> <span class="n">Bottleneck</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">pretrained</span><span class="p">,</span> <span class="n">progress</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="inception-网络">Inception 网络</h3>
<p>如果 ResNet 是为了更深，那么 Inception 家族就是为了更宽。第一个见解与对层的操作有关。在传统的卷积网络中，每一层都会从之前的层提取信息，以便将输入数据转换成更有用的表征。</p>
<p>见解 1：为什么不让模型选择？</p>
<p>这种模型架构的信息密度更大了，这就带来了一个突出的问题：计算成本大大增加。不仅大型（比如 5×5）卷积过滤器的固有计算成本高，并排堆叠多个不同的过滤器更会极大增加每一层的特征映射的数量。而这种计算成本增长就成为了我们模型的致命瓶颈。</p>
<p>这就涉及到了见解 2：使用 1×1 卷积来执行降维。为了解决上述计算瓶颈，Inception 的作者使用了 1×1 卷积来「过滤」输出的深度。一个 1×1 卷积一次仅查看一个值，但在多个通道上，它可以提取空间信息并将其压缩到更低的维度。比如，使用 20 个 1×1 过滤器，一个大小为 64×64×100（具有 100 个特征映射）的输入可以被压缩到 64×64×20。通过减少输入映射的数量，Inception 可以将不同的层变换并行地堆叠到一起，从而得到既深又宽（很多并行操作）的网络。</p>
<p>Inception Net v3 incorporated all of the above upgrades stated for Inception v2, and in addition used the following:</p>
<ul>
<li>RMSProp Optimizer.</li>
<li>Factorized 7x7 convolutions.</li>
<li>BatchNorm in the Auxillary Classifiers.</li>
<li>Label Smoothing (A type of regularizing component added to the loss formula that prevents the network from becoming too confident about a class. Prevents over fitting).</li>
</ul>
<p>关于skip-connect 的理解： skip-connect 也就是和残差连接。</p>
<p>将输入和输入的非线性变化的线性叠加。解决的是深层网络的训练问题。</p>
<h3 id="vgg网络">VGG网络</h3>
<blockquote>
<p>VGG stands for Visual Geometry Group (a group of researchers at Oxford who developed this architecture). The VGG architecture consists of blocks, where each block is composed of 2D Convolution and Max Pooling layers. VGGNet comes in two flavors, VGG16 and VGG19, where 16 and 19 are the number of layers in each of them respectively.</p>
</blockquote>
<p>在 ImageNet 大赛中，VGG 在神经网络的深度方面进行了很好的探索。完全沿用了卷积加池化堆叠的设计思路。</p>
<p>和之前 AlexNet 结构相比，无论是 VGG16 还是VGG19，其卷积池化的层数都远远高于 AlexNet 网络。</p>
<p>VGG主要由输入层、卷积层、池化层以及全连接层组成。对应的输入是224×224彩色图像</p>
<p><strong>卷积层</strong>:卷积核大小为3×3，步长设置为1，填充(padding)设置为1，并且所有隐藏层都使用ReLu作为激活函数。
<strong>池化层</strong>：每经过一次池化，图片面积变为原来的四分之一。每个池化层的池化核大小是2×2，步长为2，填充设置为0。
<strong>全连接层</strong>：VGG网络最后是三个全连接层，其中前两个全连接层均有4096个通道，第三个全连接层有1000个通道。</p>
<p>经过探索，发现16层和19层的结构效果是比较好的。所以我们现在使用最多的是VGG16和VGG19。</p>
<p>VGG的特点</p>
<ul>
<li>采用了较深的网络，最多达到19层，证明网络越深，从而准确率越高</li>
<li>串联多个小卷积，相当一个大卷积。VGG 中使用两个串联的 3* 3卷积，达到了一个 5*5 卷积的效果，但是参数量却只有之前的 9/25。同时串联多个小卷积，增加了使用 relu 非线性激活的概率，从而增加了模型的非线性特征</li>
<li>VGG16 中使用了 1*1 的卷积操作。1x1的卷积是性价比最高的卷积，可以用来实现线性变化，输出通道变换等功能，而且还可以多一次relu非线性激活。</li>
</ul>
<p>尽管VGG 在 ImageNet 中表现很好，但是将其部署到一个合适大小的GPU 上是困难的，因为需要VGG 在内存和时间上的要求很高。</p>
<p>1*1 卷积的好处</p>
<p>减少了参数，允许增加深度 ；可以降维。 Inception 块就是通过在 3<em>3 和 5</em>5 后面加入了 1*1 来减少计算； 增加了网络的表达能力。</p>
<ol start="2">
<li>ResNet 网络</li>
</ol>
<p>使用一张图表回顾一下深度网络的发展历程</p>
<img src="http://123.56.8.10:8899/images/2021/03/02/088761697df1d75104b460d2c6fae22eee005c.png" height="80%" width="80%">
<p>可以发现，网络越深，特征的等级就越高，能够解决更加抽象复杂的问题。但是当网路深度超过 20层之后，网络就难以训练。因为整个深度学习是建立在反向传播理论上的，而反向传播理论天然的缺陷是梯度消失和梯度爆炸。为了解决这个问题，科学家提出了 ResNet 网络。</p>
<p>通过研究，加入 shortcut 的连接方式就可以解决</p>
<img src="http://123.56.8.10:8899/images/2021/03/02/596d489fa4a10036.png" height ="40%" width="40%">
<p>我们可以观察到ResNet网络结构把链式法则连乘的问题变成了连加，而且有一个信号能够一直跳过N多个Shorcut结构而不衰减，使得我们并不会因为反向传播而衰减信息。</p>
<p>大量的实验证明 ResNet 的结构解决了之前“层数越深，效果越差”的问题。从某种意义上讲， ResNet 使得深度变成了可能。从而使网路深度达到了 152层，但我们实际使用中 100多层就远远足够了</p>
    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">jijeng</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2019-06-21
        
    </span>
  </p>
  
  
</div>
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    <label class="qr-code-image" for="reward">
        <img class="image" src="http://47.94.35.231:9998/blog_imgs/wechatpay.png">
        <span>微信打赏</span>
      </label>
    <label class="qr-code-image" for="reward">
        <img class="image" src="http://47.94.35.231:9998/blog_imgs/alipay.png">
        <span>支付宝打赏</span>
      </label>
  </div>
</div><footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/resnet/">resnet</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/machine_translation/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Machine_translation</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/offer_stack/">
            <span class="next-text nav-default">剑指Offer-栈、队列、链表和树</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="http://jijeng.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2018 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span>jijeng</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
